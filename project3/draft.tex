\chapter{Model-Free Deep Hedging of Variable Annuities}

\section{Introduction}

Reinforcement Learning (RL) is a branch of machine learning that focuses on training algorithms, known as agents, to make a sequence of decisions. 
The agent learns to achieve a goal in an uncertain, potentially complex environment by trial and error, using feedback from its own actions and experiences. 
Unlike supervised learning, where training data is labeled with the correct answers, in RL, an agent is provided with rewards or punishments as signals for its actions.

The core of RL revolves around the concept of the agent interacting with its environment over time, aiming to maximize the cumulative reward. 
This process involves observing the state of the environment, selecting and performing actions, and receiving rewards or penalties in response to those actions. 
The agent's objective is to learn a mapping from states to actions that maximizes this cumulative reward, which is often referred to as a policy. 
One of the fundamental frameworks for modeling RL problems is the Markov Decision Process (MDP). 
An MDP provides a mathematical formulation of the decision-making process, characterized by states, actions, rewards, and transition probabilities. 
Solving an MDP involves finding a policy that maximizes some function of the expected rewards, typically the expected cumulative reward over time.

RL algorithms can be broadly categorized into two types: model-based RL and model-free RL. 
Model-based RL utilize a model of the environment to simulate the outcomes of actions, enabling planning and decision-making with fewer interactions with the environment. 
The biggest drawback of model-based RL is model bias. 
In practice, the ground-true model is usually not available.
By using a model, an RL agent can exploit the model bias, which leads to suboptimal performance in the real environment.
Conversely, model-free RL learn directly from interactions with the environment.
Without relying on a model, model-free RL algorithms are more straightforward implement and tune.

A compelling application of RL is dynamic hedging, where the complexity and uncertainty of financial markets make traditional static models inadequate.
RL's adaptability and learning capabilities offer a promising solution to dynamically adjust hedging strategies in response to market movements and assumptions changes.
In particular, model-free RL has the potential to enhance risk management practices by developing robust strategies that can adapt in real-time.

In this paper, we propose a model-free RL algorithm to improve the risk management of variable annuities (VAs).
VAs are popular insurance products that add investment features to an insurance contract.
Hedging VAs is particularly challenging due to multiple sources of risk.
Our model-free RL approach aims to learn a hedging policy without directly accessing models of the underlying risk factors.
By interacting with an enviornment, the hedging policy can effectively manage the risks of VAs in a changing market environment.

The rest of the paper is organized as follows: 

\section{Review of Reinforcement Learning}

\subsection{Markov Decision Process}

An MDP is defined by the tuple $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$, where:

\begin{itemize}
    \item $\mathcal{S}$ is a finite set of states.
    \item $\mathcal{A}$ is a finite set of actions.
    \item $\mathcal{P}$ is a state transition probability distribution, where $\mathcal{P}(s_{t}|s_{t-1}, a_{t-1})$ represents the probability of transitioning from state $s_t$ to state $s_{t-1}$ due to action $a_{t-1}$.
    \item $\mathcal{R}$ is a reward distribution, $\mathcal{R}(s, a, s')$ is the reward received after transitioning from state $s$ to state $s'$ due to action $a$.
    \item $\gamma$ is a discount factor, $\gamma \in [0,1]$,  which models the present value of future rewards.
\end{itemize}

The objective in an MDP is to find a policy, $\pi: \mathcal{S} \rightarrow \mathbb{P}(\mathcal{A})$, that maximizes the cumulative reward over time.
A Markov Decision Process (MDP) provides a mathematical framework for solving sequential decision-making tasks in an enviornment where outcomes are partly random and partly under the control of a hedging agent. 
For hedging VAs, the asset dynamics and the mortality model are not controlled by the agent, but the agent controls the cumulative reward by deciding on the hedging weights.
In this section, we reformulate the hedging enviornment of VAs from Section~\ref{subsec:VApayout} as an MDP and define the key components of the MDP.

\textbf{Hedging Environment of Variable Annuities} \label{subsec:VASimulation}

Consider a generic VA contract with maturity $T>0$ periods, e.g., $T=240$ months.
Then the contract expires at $T'=\min\{T,\tau\}$, i.e., the earlier of the contract maturity and the death of the policyholder.
Let $S_t$, $F_t$, and $G_t$ be the indexed stock price, the subaccount value and the guarantee value, respectively, at time $t=1,2,\ldots,T$.
Evolution of the subaccount value and the guarantee value of a VA contract affect the contract payout.
For clarity, we use $F_t$ and $F_{t_+}$ to denote the sub-account value just before and just after the withdrawal at time $t$, if any.
Let $\eta_g$ be the gross rate of management fee that is deducted from the fund value at each period and let $\eta_n < \eta_g$ be the net rate of management fee income to the insurer.
The difference between the gross management fee and the net management fee income represents the incurred investment expenses.

At the inception of the contract, i.e., $t=0$, we assume that the whole premium is invested in the stock index and the guarantee base is set to the sub-account value:
\begin{equation*}
    S_0=F_0=G_0.
\end{equation*}
At each time $t=1,\ldots,T$, the following events take place in the following order:
\begin{enumerate}
    \item The dynamic lapse rate $q_t$ is applied to the contract, i.e., $q$ of the policyholders leave the contract at time $t$.
        \begin{equation} \label{eq3:lapse}
            q_t = q_t^B \cdot \text{clip} (M_q (\frac{G_{t-1}}{F_{t-1}} - D_q), L_q, U_q),
        \end{equation}
    where $q_t^B$ is the base lapse rate, $\text{clip}(x, a, b)$ is a function that clips the value of $x$ to be within the range $[a, b]$, and $M_q$, $D_q$, $L_q$, and $U_q$ are the model parameters taken from National Association of Insurance Commissionersâ€™ (NAIC) Valuation Manual 21~\citep{naic2021}.
    \item The sub-account value changes according to the growth of the underlying stock and the (gross) management fee is deducted. That is, 
        \begin{equation} \label{eq3:subaccount}
            F_t = F_{(t-1)_+}\cdot\frac{S_{t}}{S_{t-1}}\cdot(1-\eta_g)\cdot(1-q_t),
        \end{equation} 
    where $(x)^+=\max\{x,0\}$ and $F_{(t-1)_+}$ will be defined later. The insurer's income at time $t$ is the net management fee, i.e., $F_t\eta_n$. 

    \item The guarantee value ratchets up (ratcheting is a common feature in GMWB) if the sub-account value exceeds the previous guarantee value, i.e., 
        \begin{equation} \label{eq3:guarantee}
            G_t = \max\{G_{t-1}\cdot(1-q_t),F_t\}.
        \end{equation} 
    A GMMB can be modeled with $G_t = G_{t-1}\cdot(1-q_t)$.

    \item The withdrawal is made (for GMWB) and is deducted from the sub-account value, i.e., 
        \begin{equation} \label{eq3:withdrawal}
            F_{t_+} = (F_t - I_t)^+,
        \end{equation} 
    where $I_t = \xi G_t$. A GMMB can be modeled with $\xi = 0$.
\end{enumerate}

Consider a VA contract whose delta hedge portfolio at any time~$t$, $t=0,1,\ldots,T-1$, consists of $\Delta_t$ units in the underlying stock and $B_t$ amount of a risk-free zero-coupon bond maturing at time $T$.
The value of the hedge portfolio at time~$(t-1)$ is:
\begin{equation*}
    H_{t-1} = \Delta_{t-1} S_{t-1} + B_{t-1},
\end{equation*}
where $S_t$ is the underlying stock price and any time $t>0$.
This hedge portfolio is brought forward to the next rebalancing time~$t$, when its value becomes:
\begin{equation*}
    H_{t}^{bf} = \Delta_{t-1} S_{t} + B_{t-1}e^{r}.
\end{equation*}
Therefore, the time~$t$ hedging error, i.e., the cash flow incurred by the insurer due to rebalancing at time~$t$, is
\begin{equation}
    HE_t = H_t - H^{bf}_t, \quad t=1,\ldots, T-1.
\end{equation}
The P\&L of the VA contract includes the cost of the initial hedge ($H_0$), the hedging errors~\eqref{eq2:hedgingerror}, the unwinding of the hedge at maturity ($H^{bf}_T$), and the contract payout at time~$t\in \{0,\ldots,T\}$.
Mathematically, the present value of the liability for a GMMB contract is 
\begin{align} \label{eq3:lossGMMB}
L   & = H_0 - e^{-rT} H^{bf}_T + \sum_{t=1}^{T-1} e^{-rt} HE_t + \sum_{t=1}^T e^{-rt} C_t(\Delta_{t-1}) - \sum_{t=1}^T e^{-rt} F_t\eta_n + e^{-rT} (G_t - F_t)^+  \nonumber \\ 
    & = e^{-rT} (G_T - F_{T^+})^+ + \sum_{t=1}^T e^{-rt}  \left( \Delta_{t-1} (S_{t-1} - e^{-r} S_t) + C_t(\Delta_{t-1}) - F_t\eta_n \right) 
\end{align}
where the equality holds by a rearrangement of terms and a telescopic sum simplification of $e^{-rt}B_t$, $t=0,\ldots,T-1$, and $C_t(\Delta_{t-1}) := C \cdot S_{t-1} \cdot (|\Delta_{t-1}| + 0.01 \Delta_{t-1}^2)$ is the tranaction cost at time $t$~\cite{garleanu2013dynamic}.
Hence, the present liability consist of the hedging errors, the transaction costs, the net management fees, and the contract payouts.
Similarly, the present value of the liability for a GMWB contract is
\begin{equation} \label{eq3:lossGMWB}
L = \sum_{t=1}^T e^{-rt}  \left( \Delta_{t-1} (S_{t-1} - e^{-r} S_t) + C_t - F_t\eta_n \right)
\end{equation}
The GMWB payout is implicitly included in $F_t\eta_n$.
Both Equation~\ref{eq3:lossGMMB} and Equation~\ref{eq3:lossGMWB} neglect the transaction cost to liquidate the hedge portfolio at maturity.
The discrete-time hedging problem of VAs can be conveniently formulated as an MDP, where the states, actions, transition probabilities, policies, and rewards are defined as follows.

\textbf{State Space}

The state space $\mathcal{S}$ represents all the information needed to characterize the hedging environment.
In a VA hedging problem, $\mathbf{s}_t$, the information available to a hedging agent at each time $t$ is 
$$\mathbf{s}_t := (S_t, F_t, G_t, \tau, \Delta_{t-1}) \in \mathcal{S}.$$
where $\tau$ is the remaining time to maturity and $\Delta_{t-1}$ is the previous hedging weight.
The hedging environment is partially observed, e.g., the agent does not have access to the contract specifications.
It has to learn a representation of the relevant information from the observed state $\mathbf{s}_t$.

\textbf{Action Space}

The action space $\mathcal{A}$ represents the set of actions that the agent can take at each time $t$.
In a VA hedging problem, the action space only includes the hedging weight $\Delta_t$.

\textbf{Policy}

A policy $\pi$ determines the best action to take in a given state, i.e.,
$$a_t \sim \pi(\cdot|\mathbf{s}_t).$$
In a VA hedging problem the policy outputs the hedging weight $a_t = \Delta_t$ given the observed state $\mathbf{s}_t$.
In the following sections, we will refer to $\Delta_t$ as the policy.

\textbf{Transition Probabilities}

The transition probabilities $\mathcal{P}$ represent the probability of transitioning from one state to another from taking an action.
In our setup, the transition probabilities are fully determined by Equation~\ref{eq3:lapse}, Equation~\ref{eq3:subaccount}, Equation~\ref{eq3:guarantee}, and Equation~\ref{eq3:withdrawal}.
Let $\mathcal{P}(\mathbf{s}_{t+1}|\mathbf{s}_t, a_t)$ be the probability of transitioning from state $\mathbf{s}_t$ to state $\mathbf{s}_{t+1}$ due to action $a_t$.
Aside from the previous hedging weight $\Delta_{t-1}$, the transition probabilities are independent from the actions.
Paths simulated from the transition probabilities are called trajectories (or episodes) of the MDP.

\textbf{Reward and Discount Factor}
The reward function $\mathcal{R}$ and discount factor $\gamma$ in a typical RL problem are defined as follows:
\begin{align}
    \mathcal{R} & = \sum_{t=1}^{T-1} \gamma^t R_t \\
    \gamma      & = e^{-r}, \nonumber
\end{align}
where $R_t$ is the term reward received by the agent at time $t$, and the discount factor $\gamma$ is conveniently inherited from the risk-free rate $r$. 
In hedging, the term reward $R_t$ should be drived from Equation~\ref{eq3:lossGMMB} and Equation~\ref{eq3:lossGMWB} to represent the performance of the hedging policy, i.e., how closely the hedging portfolio tracks the liability.
For GMWB, we propose a term reward formulation that encourages the agent to maximize the expected reward with low hedging errors and transaction costs:
\begin{equation} \label{eq3:rewardGMWB}
    R_t = F_{t-1}\eta_n - \Delta_{t-1} |S_{t-1} - e^{-r}S_{t}| - C_{t-1}(\Delta_{t-1}), \quad t \in \{1,\ldots,T \}
\end{equation}
The reward function of GMWB directly links to the time-$t$ loss of a GMWB contract as in Equation~\ref{eq3:lossGMWB}.
The GMMB loss consists of an additional payoff at maturity $T$, thus its term reward is formulated as:
\begin{equation} \label{eq3:rewardGMMB}
    R_t = 
    \begin{cases}
    F_{t-1}\eta_n - \Delta_{t-1} |S_{t-1} - e^{-r}S_{t}| - C_{t-1}(\Delta_{t-1}),                         & t\in \{1,\ldots,T-1 \} \\
    F_{t-1}\eta_n - \Delta_{t-1} |S_{t-1} - e^{-r}S_{t}| - C_{t-1}(\Delta_{t-1}) + (G_T - F_{T^+})^+      & t = T
    \end{cases}
\end{equation}
It is worth noting that learning to hedge GMWB is easier for an RL agent due to the absense of reward at maturity (long-term feedback).

\subsection{Model-Based RL: From AlphaZero to Deep Hedging}
Deep reinforcement learning first attacted attentions when AlphaGo~\citep{silver2016mastering} surpasses human in playing Go. 
Soon after, AlphaZero~\citep{silver2016mastering} emerged as a model-based RL algorithm that improves over self-plays.
It quickly extended beyond Go to other games like Chess and Shogi.
In essence, the AlphaZero agent use a neural network to paramaterize the policy and the value function.
The neural network takes the current state of the game as input and outputs the probability distribution of the next move and a value of the current state.
\begin{equation}
    (\mathbf{\pi}, v) = f_{\theta}(s), \quad L(\theta) = - \mathbf{\pi}^{\intercal} \log(\mathbf{p}) + (z - v)^2  + c||\theta||^2,
\end{equation}
where $f_{\theta}$ is the neural network with parameters $\theta$, $s$ is the current state of the game, $\pi$ is the policy, $v$ is the value of the current state, and $c$ is a regularization parameter.
$\mathbf{p}$ and $z$ are the probability distribution of the next move and the outcome of the game, respectively.
They are induced by a planning algorithm, i.e., Monte Carlo Tree Search (MCTS).
Since it is infeasible to efficiently simulate trajectories in the game of Go, a planning algorithm is necessary to generate expected outcomes.
The neural network $f_{\theta}$ is trained with a stochastic gradient descent algorithm to minimize $L$.
$L$ has three components.
The last term penalizes large neural network weights.
Its first two terms measure the similarity between $f_{\theta}(s)$, the learned policy and value function, with the action and game outcome planned by the MCTS.

In a typical hedging problem, the transition probabilities are independent from the hedging weights, i.e., the actions.
This simplifies the training of the neural network as complete trajectories can be simulated without the need of a planning algorithm.
In contrast to AlphaZero, deep hedging~\citep{buehler2019deep} uses $f_{\theta}$ for planning, as opposed to the MCTS.
More specifically, at any time $t$, the neural network $f_{\theta}$ serves as a model that predict possible future rewards before they are actually observed.
This is known as model-based RL. 
Using a model allows for planning, i.e., decision-making take place using future situations that have not been experienced by the agent.

Let $(\tilde{\mathbf{s}}_0, \tilde{\Delta}_0, \tilde{\mathbf{s}}_1, \tilde{\Delta}_1, \ldots, \tilde{s}_T)$ and $\tilde{\mathcal{R}} = (R_1, \ldots, R_t)$ be a trajectory and the hedging reward of the MDP, respectively.
The neural network $f_{\theta}$ takes the previous state and hedging weight as input and outputs the hedging weight at the current state.
\begin{equation}
    \mathbf{\Delta} = f_{\theta}(\mathbf{s}_0, \ldots, \mathbf{s}_T), \quad L(\theta) = \rho(L) = \rho(-\mathcal{R}) = \rho(- \sum_{t=0}^{T} \gamma^t R_t),
\end{equation}
where $\rho(L)$ is a convex risk measure of the total present liability $L$. 
A common choice of $\rho$ is a CVaR.
In practice, the neural network $f_{\theta}$ is trained to minimize the empirical CVaR over a batch of trajectories.

\subsection{Model-Free RL}

\section{Problem Formulation}

\section{Numerical Experiments}

All numerical experiments are run using~\cite{paszke2019pytorch}.

\section{Conclusion}


