\chapter{Transfer Learning for Rapid Adaptation of Deep Neural Network Metamodels in Dynamic Hedging of Variable Annuities} \label{chap:project3}

\section{Introduction}

In the evolving landscape of financial markets, insurance products such as \gls{va}s have gained a substantial amount of interest due to their ability to provide both investment growth and guaranteed benefits. 
Managing risks associated with these products in volatile market conditions is a complex task that demands sophisticated financial modeling techniques such as nested simulation.
A standard nested simulation procedure involves running a large number of simulations to generate a dataset of scenario-wise contract losses with two levels of simulations.
Practitioners can estimate tail risk measures using the realized losses for different risk scenarios obtained from these simulations.
Nested simulation is extremely computationally expensive.
To address the computational burden, metamodeling techniques have been proposed, where a surrogate model approximates the outcomes of a complex simulation procedure. 
Traditional \gls{ml} models often struggle to capture the intricate nonlinear relationships and temporal dependencies inherent in financial data.
In particular, \gls{dnn}s, and specifically \gls{lstm} networks, have been employed as metamodels to predict the outcomes of the inner simulations efficiently.
Chapter~\ref{chap:project2} introduced a metamodeling-based nested simulation framework for dynamic hedging of \gls{va}s that approximates the inner simulation by an \gls{lstm} metamodel.
It shows that \gls{lstm}s, as a type of gated \gls{rnn}s, were well-suited as metamodels for \gls{mc} simulation of financial time series and could capture the long-term dependencies in the simulation dataset.
This property is crucial for the application of dynamic hedging of \gls{va}s.

Despite the advantages of using \gls{dnn} metamodels, computational challenges arise when market conditions change or new \gls{va} contracts with different features are introduced. 
Retraining neural network metamodels from the scratch in response to every change is computationally inefficient and time-consuming.
Practitioners with limited resources face several challenges:
\begin{enumerate}
    \item The simulation budget is limited, but \gls{dnn} metamodels require extensive training data to learn the feature-label relationship~\citep{golestaneh2024many}. 
    \item The computational cost of retraining a suitable \gls{dnn} metamodel is often much higher than fitting a traditional parametric regression.
    \item Financial markets are dynamic with frequent shifts in volatility, interest rates, and other risk factors~\citep{cont2001empirical}.
\end{enumerate}

Constraints on the simulation budget is the most critical challenge for dynamic hedging of \gls{va}s.
Any method that involves nested simulation is computationally expensive, and the computational cost is often prohibitive for changing market conditions.
Therefore, it is essential to develop methods that can rapidly adapt existing metamodels to new conditions without incurring the full computational cost of retraining and extensive addtional simulations.
Transfer Learning (\gls{tl}) offers a compelling solution to this problem by enabling the reuse of a pre-trained model on a new but related task~\citep{pan2009survey}.
In the context of \gls{dnn}s, \gls{tl} involves leveraging the knowledge acquired during training on one dataset to improve learning performance on a different dataset~\citep{yosinski2014transferable}.
This approach can substantially reduce the amount of training data needed, training time and computational resources while enhancing model generalization.
Instead of starting from the scratch, a new \gls{dnn} metamodel can be built on top of a pre-trained metamodel and fine-tuned on new data.
This allows the metamodel to adapt quickly to unseen conditions and new \gls{va} contracts.

In this chapter, we explore the application of \gls{tl} to the dynamic hedging of \gls{va}s using \gls{lstm} metamodels.
We propose a novel \gls{tl} framework that accelerates the training of \gls{dnn} metamodels for nested simulation procedures in dynamic hedging.
This setting is particularly relevant for dynamic hedging problems, as insurance companies often underwrite new contracts under changing market conditions and actuarial assumptions.
This chapter proposes a \gls{tl} framework that makes fast adaptation of metamodels possible.
Our approach involves pre-training an \gls{lstm} network on a large dataset of \gls{va} simulations and then fine-tuning it on a smaller dataset of new \gls{va} contracts.
We evaluate the performance of the \gls{tl} framework on a real-world dataset of \gls{va} contracts and compare it with training a \gls{lstm} metamodel from scratch.
More importantly, we show the strengths and weaknesses of different \gls{tl} techniques for rapid adaptation of \gls{lstm} metamodels.

The rest of this chapter is organized as follows.
Section~\ref{sec3:background} provides an overview of the dynamic hedging problem for \gls{va}s and the use of \gls{lstm} networks as metamodels in a nested simulation procedure.
Section~\ref{sec3:transfer_learning} introduces the \gls{tl} framework for rapid adaptation of \gls{lstm} metamodels in dynamic hedging.
Section~\ref{sec3:experiments} presents the experimental setup and results, comparing the performance of \gls{tl} with training from scratch.
Finally, Section~\ref{sec3:conclusion} concludes the chapter and discusses future research directions.

\section{Transfer Learning in Financial Metamodeling} \label{sec3:background}

\gls{tl} is a machine learning paradigm where knowledge acquired from a source task is utilized to improve learning performance on a related target task.
One of the primary applications of \gls{tl} in finance is asset price prediction. 
Traditional models, such as geometric Brownian motion (\gls{gbm}), autoregressive integrated moving average (\gls{arima}) and generalized autoregressive conditional heteroskedasticity (\gls{garch}), have been widely used in time series modeling.
\gls{arima} models~\citep{box1970distribution} combine autoregression, differencing, and moving average components to capture linear relationships in time series data.
\gls{garch} models~\citep{bollerslev1990modelling}, extend \gls{arch} models to better capture volatility clustering, which is a key stylized fact of financial time series~\citep{cont2001empirical}.
However, these models often struggle to capture complex dynamics, e.g., regime changes~\citep{hamilton1989new} and extreme market events~\citep{embrechts2013modelling} that are prevalent in financial markets.
In Chapter~\ref{chap:project2}, our numerical experiment uses a regime-switching geometric Brownian motion (\gls{rsgbm}) as the underlying asset model.
For estimating tail risk measures, a metamodel-based nested simulation procedure is proposed to approximate the inner simulation.
We have shown \gls{dnn} metamodels like \gls{rnn} and \gls{lstm} networks have demonstrated substantial improvements in modeling temporal dependencies, and they save a substantial amount of simulation budget.
However, training these models from the scratch requires an extensive amount of simulation data, which may not always be available for specific assets or under certain market conditions.

\gls{tl} offers a solution to this problem by leveraging knowledge from related assets or tasks to improve the learning performance of \gls{dnn} metamodels on the target task.
In algorithmic trading,~\cite{jeong2019improving} used \gls{tl} to enhance the performance of a reinforcement learning agent by preventing overfitting from an insufficient amount of market data.
\gls{tl} techniques have also been applied in building fraud detection systems. 
Financial fraud often exhibits subtle and evolving patterns, and it is challenging to develop robust detection models.
By transferring previous knowledge from detected fraud cases, models can adapt to detect new fraud schemes more effectively~\citep{lebichot2021transfer}.
\cite{yan2024comprehensive} conduct a comprehensive survey study of current \gls{tl} techniques in financial applications, and they find almost all applications of \gls{tl} only employ parameter transfer, where the pre-trained model is fine-tuned on the target task.
Our multi-task learning framework in Section~\ref{sec3:transfer_learning} extends beyond parameter transfer to shared representation learning across multiple tasks.

Regarding a nested simulation of \gls{va}s,~\cite{cheng2019fast} is the most relevant study to our work, where they proposed a \gls{tl} framework for fast valuation of large portfolios of \gls{va}s.
Instead of using stochastic kriging~\citep{gan2015valuation}, they employed a pre-trained \gls{dnn} to select the best representative scenarios from a large portfolio of \gls{va}s.
In our work, we focus on the application of \gls{tl} to accelerate the training of \gls{lstm} metamodels for a nested simulation in dynamic hedging of \gls{va}s.
The fine-tuned \gls{lstm} metamodels can be readily adapted to the two-stage procedure and the single-stage procedure in Chapter~\ref{chap:project2} to predict the contract losses under different scenarios.

In our context of \gls{dnn}s metamodeling-based simulation procedures for hedging \gls{va}s, \gls{tl} involves pre-training a \gls{dnn} metamodel where the simulation budget is abundant and then fine-tuning it on a smaller dataset of new contracts or market conditions.
Written on the same underlying asset, different \gls{va} contracts may share common features or exhibit similar patterns, especially temporal dependencies and regime changes in the underlying financial time series.
Similarly, two \gls{va}s with the same features but based on different underlying assets may have some shared characteristics that can be leveraged to improve the learning performance of the metamodel.
By transferring knowledge from a pre-trained model to a new but related task, \gls{tl} can substantially reduce the computational cost of training the metamodel on the target task.

\gls{lstm} networks are well-suited for modeling sequential data due to their ability to capture long-term dependencies~\citep{hochreiter1997long}.
For the application of \gls{va} risk management using metamodel-based nested simulation, \gls{lstm} networks approximate the inner simulation, i.e., the mapping from scenarios to the scenario-wise contract losses.
In Chapter~\ref{chap:project2}, we treat metamodeling as a supervised learning problem and demonstrated that \gls{lstm} networks could effectively model this complex relationships with extensive training on a large dataset of \gls{va} simulations.
The total computational cost originates from two sources: running the standard nested simulation procedure to generate the training data and training the \gls{lstm} network on the generate dataset.
Given a new \gls{va} contract, these steps need to be repeated to adapt the \gls{lstm} metamodel to new market conditions and contract features.
For a \gls{dnn} with a large number of parameters, retraining the \gls{lstm} network from the scratch can be computationally expensive.
Generating a large training dataset using the standard nested simulation procedure is costly, and the computational burden can be prohibitive for certain assets or market conditions.
\gls{tl} offers an alternative approach that keeps the previous knowledge, i.e., the pre-trained model, as a foundation.
Building on the pre-trained model, the \gls{lstm} network can be fine-tuned on a smaller dataset of new contracts or market conditions.
This approach accelerates the adaptation of the \gls{lstm} metamodel to new conditions.
Computational savings come in two forms: 
\begin{enumerate}
    \item   fine-tuning requires less training time than training \gls{lstm} metamodels from scratch, and
    \item   fewer training data points are needed to achieve a good \gls{lstm} metamodel, which is particularly beneficial when the standard nested simulation procedure is costly to implement.
\end{enumerate}

In supervised learning, a \textbf{domain} $\mathcal{D}$ comprises a feature space $\mathcal{X}$ and a marginal probability distribution $F$. Correspondingly, a \textbf{task} $\mathcal{T}$ consists of a label space $\mathcal{Y}$ and a predictive function $f: \mathcal{X} \rightarrow \mathcal{Y}$ that maps input features to output labels.

A typical \gls{tl} framework for supervised learning consists of the following:
\begin{itemize}
    \item   \textbf{source domain}: $\mathcal{D}_{\text{So}} = \{\mathcal{X}_{\text{So}}, F_{\text{So}}(\mathbf{X})\}$,
    \item   \textbf{source task}: $\mathcal{T}_{\text{So}} = \{\mathcal{Y}_{\text{So}}, f_{\text{So}}(\cdot)\}$,
    \item   \textbf{target domain}: $\mathcal{D}_{\text{Ta}} = \{\mathcal{X}_{\text{Ta}}, F_{\text{Ta}}(\cdot)\}$,
    \item   \textbf{target task}: $\mathcal{T}_{\text{Ta}} = \{\mathcal{Y}_{\text{Ta}}, f_{\text{Ta}}(\cdot)\}$,
\end{itemize}
where $\mathcal{X}_{\text{So}}$ and $\mathcal{X}_{\text{Ta}}$ include input features derived from the outer simulation of two different \gls{va} contracts or market conditions, and $F_{\text{So}}$ and $F_{\text{Ta}}$ are the marginal probability distributions of the source and target domains, respectively.
Hence, the source domain and task are related to one \gls{va} contract for a certain market condition, while the target domain and task are related to new \gls{va} contracts and market conditions for which we don't have the simulation budget to run the standard nested simulation procedure extensively.
In our numerical experiments, the dataset is generated by running the standard nested simulation procedure in Algorithm~\ref{alg2:standardProcedure} for a large number of \gls{va} contracts.
The input features $\mathbf{X}$ are the return vectors from Equation~\eqref{eq2:return}, and the output labels $L$ are the contract losses for different outer scenarios.
The stock price vector $\mathbf{S}$ is not crucial for the discussion of this chapter and is omitted since it can be immediately recovered from $\mathbf{X}$ and the initial stock price $S_0$.
We keep the number of features to be $\num{240}$ for both source and target domains as in Section~\ref{sec2:numerical}.
The predictive function $f_{\text{So}}$ and $f_{\text{Ta}}$ are trained to estimate outputs $L_{\text{So}}$ and $L_{\text{Ta}}$, i.e., approximating the inner simulations under the source and target tasks, respectively.
\gls{tl} seeks to improve the learning of the target predictive function $f_{\text{Ta}}(\cdot)$ in $\mathcal{D}_{\text{Ta}}$ by leveraging the previous training on $\mathcal{D}_{\text{So}}$ and $f_{\text{So}}(\cdot)$, particularly when $\mathcal{D}_{\text{So}} \neq \mathcal{D}_{\text{Ta}}$ or $\mathcal{T}_{\text{So}} \neq \mathcal{T}_{\text{Ta}}$.
In Chapter~\ref{chap:project2}, we have $\mathcal{D}_{\text{So}} = \mathcal{D}_{\text{Ta}}$ and $\mathcal{T}_{\text{So}} = \mathcal{T}_{\text{Ta}}$.

In our \gls{tl} setting, both the source and target tasks involve learning a mapping from the risk factors to the \gls{va} contract losses.
For the source task, we have a dataset $\mathcal{D}_{\text{So}} = { (X_{\text{So}}^{(i)}, L_{\text{So}}^{(i)}) }_{i=1}^{M_{\text{So}}}$, where $M_{\text{So}}$ is the number of training samples, i.e., the number of outer simulation paths used to generate the dataset with a standard nested simulation procedure.
$X_{\text{So}}^{(i)} \in \mathcal{X}_{\text{So}}$ and $L_{\text{So}}^{(i)} \in \mathcal{Y}_{\text{So}}$.
The ultimate objective is to learn a metamodel $f_{\text{Ta}}$ that predicts the \gls{va} contract losses $L_{\text{So}}$ from the scenarios $X_{\text{Ta}}$ in the form of a financial time series.
\gls{tl} starts by training a metamodel $f_{\text{So}}$ on the source domain $\mathcal{D}_{\text{So}}$.
For both the two-stage procedure and the single-stage procedure in Chapter~\ref{chap:project2}, $f_{\text{So}}$ is approximated by an \gls{lstm} network $f_{\text{So}}(\cdot ; \theta_{\text{So}})$, where the parameters of the \gls{lstm} network, $\theta_{\text{So}}$ are learned by minimizing an \gls{mse} loss function on the source domain.
Then the pre-trained parameters $\theta_{\text{So}}$ to inform the learning of $f_{\text{Ta}}(\cdot ; \theta_{\text{Ta}})$ on the target domain $\mathcal{D}_{\text{Ta}}$.
The training on the target domain should encourage similarity between $\theta_{\text{So}}$ and $\theta_{\text{Ta}}$ to facilitate the transfer of knowledge.

The most common \gls{tl} techniques for supervised learning include fine-tuning, layer freezing, and multi-task learning. 
These techniques can be categorized based on how they leverage pre-trained metamodels and how they encourage the similarity between $f_{\text{So}}(\cdot ; \theta_{\text{So}})$ and $f_{\text{Ta}}(\cdot ; \theta_{\text{Ta}})$.

\subsection{Fine-tuning}

Fine-tuning is a commonly used \gls{tl} technique that uses the \textbf{same neural network architecture} for both the source and target tasks.
We first train the source \gls{lstm} metamodel $f_{\text{So}}(\cdot; \theta_{\text{So}})$ on $\mathcal{D}_{\text{So}}$, capturing temporal dependencies and patterns relevant to the source task. 
For the target task, we initialize $\theta_{\text{Ta}} = \theta_{\text{So}}$ and proceed to fine-tune the entire network on $\mathcal{D}_{\text{Ta}}$ by using a smaller learning rate. 

\begin{algorithm}[ht!]
\caption{Fine-tuning Metamodel for a Target Task}
\begin{algorithmic}[1] \label{alg3:fineTuning}
    \STATE \textbf{Input:} source dataset $\mathcal{D}_{\text{So}} = \{(X_{\text{So}}^{(i)}, L_{\text{So}}^{(i)})\}_{i=1}^{M_{\text{So}}}$, target dataset $\mathcal{D}_{\text{Ta}} = \{(X_{\text{Ta}}^{(i)}, L_{\text{Ta}}^{(i)})\}_{i=1}^{M_{\text{Ta}}}$, learning rate $\alpha_{\text{So}}$ and smaller learning rate $\alpha_{\text{Ta}}$.
    
    \STATE Train a \gls{lstm} metamodel $f_{\text{So}}(\cdot; \theta_{\text{So}})$ on $\mathcal{D}_{\text{So}}$:
    \begin{equation}
        \theta_{\text{So}} = \min_{\theta} \frac{1}{M_{\text{So}}} \sum_{i=1}^{M_{\text{So}}} \left( f_{\text{So}}(X_{\text{So}}^{(i)}; \theta) - L_{\text{So}}^{(i)} \right)^2.
    \end{equation}

    \STATE Initialize the target metamodel parameters $\theta_{\text{Ta}}$ using the pre-trained metamodel parameters:
    \[
    \theta_{\text{Ta}} \gets \theta_{\text{So}}.
    \]
    
    \STATE Fine-tune the entire \gls{lstm} model $f_{\text{Ta}}(\cdot; \theta_{\text{Ta}})$ on the target dataset $\mathcal{D}_{\text{Ta}}$ using a smaller learning rate $\alpha_{\text{Ta}}$:
    \begin{equation}
        \theta_{\text{Ta}} = \min_{\theta} \frac{1}{M_{\text{Ta}}} \sum_{i=1}^{M_{\text{Ta}}} \left( f_{\text{Ta}}(X_{\text{Ta}}^{(i)}; \theta) - L_{\text{Ta}}^{(i)} \right)^2.
    \end{equation}
    
    \STATE \textbf{Output:} Final adapted \gls{lstm} metamodel $f_{\text{Ta}}(\cdot; \theta_{\text{Ta}})$ for the target task.
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg3:fineTuning} assumes that learned sequential representations are beneficial for the target task.
Only minor adjustments are needed to adapt the metamodel to the new task, and the training process is accelerated by the pre-trained parameters.
Fine-tuning is particularly useful when the nested simulation procedures for the two \gls{va} contracts are closely related.
Only minor adjustments are needed to adapt the \gls{lstm} metamodel to the new domain.

\subsection{Layer Freezing}

In a layer freezing approach, we partition the model parameters into frozen parameters $\theta_0$ and trainable parameters $\theta_1$, such that $\theta = [\theta_0, \theta_1]$. 
Typically, $\theta_0$ are parameters of the lower layers, and $\theta_1$ are parameters of the higher layers that include the output layer.
The intuition behind layer freezing is that the lower layers capture general features that are transferable across tasks, while the higher layers are more task-specific.

\begin{algorithm}[ht!]
    \caption{Layer Freezing for Metamodel Transfer}
    \begin{algorithmic}[1] \label{alg3:layerFreezing}
        \STATE \textbf{Input:} source dataset $\mathcal{D}_{\text{So}} = \{(X_{\text{So}}^{(i)}, L_{\text{So}}^{(i)})\}_{i=1}^{M_{\text{So}}}$, target dataset $\mathcal{D}_{\text{Ta}} = \{(X_{\text{Ta}}^{(i)}, L_{\text{Ta}}^{(i)})\}_{i=1}^{M_{\text{Ta}}}$, learning rates $\alpha_{\text{So}}$ and $\alpha_{\text{Ta}}$, frozen parameters $\theta_0$, trainable parameters $\theta_1$.
        
        \STATE Train \gls{lstm} model $f_{\text{So}}(\cdot; \theta_{\text{So}})$ on $\mathcal{D}_{\text{So}}$:
        \begin{equation}
            \theta_{\text{So}} = [\theta_0, \theta_1] = \min_{\theta} \frac{1}{M_{\text{So}}} \sum_{i=1}^{M_{\text{So}}} \left( f_{\text{So}}(X_{\text{So}}^{(i)}; \theta) - L_{\text{So}}^{(i)} \right)^2.
        \end{equation}
        
        \STATE Initialize the target model parameters $\theta_{\text{Ta}} = [\theta_0, \theta_1]$ using the pre-trained source model parameters $\theta_{\text{So}}$:
        \[
        \theta_{\text{Ta}} \gets \theta_{\text{So}} = [\theta_0, \theta_1].
        \]
        
        \STATE Freeze the parameters of the shared layers $\theta_0$:
        
        \STATE Fine-tune the trainable layers $\theta_1$ on the target dataset $\mathcal{D}_{\text{Ta}}$ using Algorithm~\ref{alg3:fineTuning}:
        \begin{equation}
            \theta_{\text{Ta}} = \min_{\theta_1} \frac{1}{M_{\text{Ta}}} \sum_{i=1}^{M_{\text{Ta}}} \left( f_{\text{Ta}}(X_{\text{Ta}}^{(i)}; [\theta_0, \theta_1]) - L_{\text{Ta}}^{(i)} \right)^2.
        \end{equation}
        
        \STATE \textbf{Output:} Final adapted \gls{lstm} metamodel $f_{\text{Ta}}(\cdot; [\theta_0, \theta_1])$ for the target task.
    \end{algorithmic}
    \end{algorithm}

\subsection{Multi-task Learning}

Multi-task learning~\citep{caruana1997multitask} refers to a machine learning paradigm where a single model is trained simultaneously on multiple related tasks.
Shared representations are learned across tasks, which can improve learning efficiency and predictive performance on each individual task with limited data.
In contrast to fine-tuning and layer freezing, multi-task learning aims at leveraging learned knowledge from multiple tasks, and all tasks are trained simultaneously.


Let $\{\mathcal{T}_k\}_{k=1}^K$ represent a set of $K$ related tasks, each corresponding to a metamodeling task for a different standard nested simulation procedure.
For each task $\mathcal{T}_k$, we have a dataset $\mathcal{D}_k = { (X_k^{(i)}, L_k^{(i)}) }{i=1}^{M_k}$, where $M_k$ is the number of training samples for task $k$.
$X_k^{(i)}$ and $L_k^{(i)}$ are the features and contract loss labels for task $k$, respectively.

For the implementation of our metamodeling studied in Chapter~\ref{chap:project2}, we consider a multi-task learning framework where the \gls{lstm} layers are shared across multiple tasks, and each task has its own fully connected layer for prediction.
The network parameters are divided into shared parameters $\theta_0$ (\gls{lstm} layers) and task-specific parameters $\theta_k$ (fully connected layers for task $k$). 
As opposed to layer freezing, all parameters are trainable.

The objective function for multi-task learning is the sum of the loss functions of all tasks:

\begin{equation} 
    \min_{\theta_0, \theta_1, \dots, \theta_K} = \sum_{k=1}^K \frac{1}{M_k} \sum_{i=1}^{M_k} \left( f_i(X_k^{(i)}; \theta_0, \theta_1, \dots, \theta_K) - L_k^{(i)} \right)^2,
\end{equation}
where \gls{mse} loss function is used as the error metric, and $f_i(\cdot; \theta_0, \theta_1, \dots, \theta_K)$ is the output of the network for task $k$.
In essence, multi-task learning uses a multi-head architecture, where each task has its own output head, but the shared \gls{lstm} layers learn a common representation across tasks.
Transfer occurs through the shared \gls{lstm} layers $\theta_0$. 
These layers learn representations of temporal patterns and dependencies common to all tasks, effectively \textbf{pooling} information from multiple simulation schemes. 
The task-specific fully connected layers $[\theta_1, \dots, \theta_K]$ allow each task to capture unique characteristics not shared with other tasks.


\begin{algorithm}
    \caption{Multi-task Learning Framework for \gls{lstm} Metamodels}
    \begin{algorithmic}[1] \label{alg3:multiTaskLearning}
        \STATE \textbf{Input:} learning rate $\alpha$, set of $K$ tasks $\{\mathcal{T}_k\}_{k=1}^K$ with datasets $\mathcal{D}_k = \{(X_k^{(i)}, L_k^{(i)})\}_{i=1}^{M_k}$, task-specific parameters $\theta_k$ for each task $k$, and shared parameters $\theta_0$.
    
        \STATE Train the multi-head \gls{lstm} metamodel on all $K$ tasks simultaneously by minimizing the multi-task loss function:
        \begin{equation} \label{eq3:multiTaskLoss}
            \min_{\theta_0, \{\theta_k\}_{k=1}^K} \sum_{k=1}^K \frac{1}{M_k} \sum_{i=1}^{M_k} \left( f_i(X_k^{(i)}; \theta_0, \theta_k) - L_k^{(i)} \right)^2.
        \end{equation}
    
        \STATE Update both the shared parameters $\theta_0$ and task-specific parameters $\{\theta_k\}_{k=1}^K$ simultaneously using backpropagation and gradient descent with learning rate $\alpha$.
        
        \STATE \textbf{Output:} Trained multi-task \gls{lstm} metamodel $f(\cdot; \theta_0, \{\theta_k\}_{k=1}^K)$ for all $K$ tasks.
    \end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg3:multiTaskLearning} outlines the multi-task learning framework for \gls{lstm} metamodels for nested simulation procedures.
In Chapter~\ref{chap:project1} and Chapter~\ref{chap:project2}, we demonstrated that pooling using a metamodel could improve the computation efficiency and estimator accuracy in nested simulation procedures.
Here, pooling happens at a higher level, where information from multiple metamodels is shared to improve the learning performance on individual tasks.
The multi-task loss function encourages the shared \gls{lstm} layers to learn generalizable representations of temporal dependencies that are beneficial across all tasks.

Choosing appropriate simulation schemes is crucial for the success of multi-task learning. 
The tasks should be related to ensure that the shared layers can capture common features beneficial across tasks. 
Criteria for selecting tasks with similar simulation schemes include:

\begin{itemize} 
    \item   \textbf{Similarity in contract specifications:} 
            simulations involving \gls{va} contracts with similar features are likely to share underlying risk factors and policyholder behaviors. 
    \item   \textbf{Similarity in underlying assets:} 
            datasets simulated under different but related asset models can provide diverse information that enrich the shared representations.
            Datasets simulated under extreme market conditions can help the shared layers learn to hedge against tail risks.
    \item   \textbf{Temporal Dynamics:} 
            \gls{va} contracts with comparable maturity and rebalance frequency can help the shared layers learn temporal dependencies consistent across tasks. 
\end{itemize}

Suppose that we aim at developing an \gls{lstm} metamodel for a \gls{gmwb} contract under a stochastic volatility asset model, but we have limited data for this simulation scheme. 
For multi-task learning, we can select related nested simulation procedures with relatively abundant data, such as:

\begin{itemize} 
    \item \gls{gmmb} contracts under a stochastic volatility asset model. 
    \item \gls{gmwb} contracts under a Black-Scholes asset model. 
    \item \gls{gmwb} contracts under a stochastic volatility model with different simulation parameters.
\end{itemize}

These tasks need to share similarities in contract features and market dynamics so that the shared \gls{lstm} layers can learn relevant temporal patterns applicable to the target task.
Otherwise, the shared layers may not be able to learn generalizable representations of temporal dependencies, and the multi-task learning framework may lead to negative transfer.
Negative transfer refers to the phenomenon where the performance of the target task is worse than that of training directly on the target task without \gls{tl}.
It occurs when the tasks are not related or an improper \gls{tl} technique is used.
Positive transfer, on the other hand, refers to the phenomenon where the performance of the target task is better than that of training directly on the target task without \gls{tl}.
Training a multi-task \gls{lstm} metamodel benefits from having more training data, and the shared layers can learn more generalizable representations of temporal dependencies.
Furthermore, if a new but similar task is introduced, fine-tuning, layer freezing, and multi-task learning can be combined to leverage the pre-trained model effectively.
More specfically, the shared layers can be frozen, and the task-specific layers for a similar task in the training set can be fine-tuned on the new task.
This shared knowledge helps the model generalize better on a target task, as the \gls{lstm} layers have been exposed to a wider variety of patterns and dynamics.

\subsection{Rapid Adaptation of \gls{lstm} Metamodels} \label{sec3:transfer_learning}

In this section, we propose a \gls{tl} framework for rapid adaptation of \gls{lstm} metamodels in dynamic hedging of \gls{va}s.
The goal is to leverage the knowledge acquired during training on a large dataset of \gls{va} simulations to improve the learning performance on a smaller dataset of new \gls{va} contracts.


\begin{algorithm}
    \caption{Transfer Learning Framework for \gls{lstm} Metamodels: Combining Fine-tuning, Layer Freezing, and Multi-task Learning}
    \begin{algorithmic}[1] \label{alg3:combined}
        \STATE \textbf{Input:} Set of $K$ tasks $\{\mathcal{T}_k\}_{k=1}^K$, with datasets $\mathcal{D}_k = \{(X_k^{(i)}, L_k^{(i)})\}_{i=1}^{M_k}$ for each task $k$, target dataset $\mathcal{D}_{\text{Ta}}$, learning rate $\alpha_{\text{So}}$ and $\alpha_{\text{Ta}}$, shared parameters $\theta_0$, task-specific parameters $\theta_k$ for each task $k$.
        
        \STATE \textbf{Multi-task learning:} define shared LSTM layers $\theta_0$ and task-specific fully connected layers $\theta_k$ for each task $k$. The LSTM layers are shared across all tasks $\{\mathcal{T}_k\}_{k=1}^K$.
        
        \STATE Train the multi-task \gls{lstm} metamodel on all $K$ tasks simultaneously:
        \begin{equation}
            \min_{\theta_0, \{\theta_k\}_{k=1}^K} \sum_{k=1}^K \frac{1}{M_k} \sum_{i=1}^{M_k} \left( f_i(X_k^{(i)}; \theta_0, \theta_k) - L_k^{(i)} \right)^2.
        \end{equation}
        
        \STATE For a related task of hedging a new \gls{va} contract, combine fine-tuning and layer freezing:

        \STATE \textbf{Layer freezing:} once the multi-task metamodel is trained, freeze the parameters of the shared LSTM layers $\theta_0$.
        
        \STATE \textbf{Fine-tuning:}  based on task similarity, initialize the target task model parameters $\theta_{\text{Ta}}$ using parameters $\theta_k$ from task $k$.
        \[
        \theta_{\text{Ta}} \gets \theta_k 
        \]
        
        \STATE Fine-tune only $\theta_{\text{Ta}}$ on the new target dataset $\mathcal{D}_{\text{Ta}}$ using a smaller learning rate $\alpha_{\text{Ta}}$:
        \begin{equation}
            \min_{\theta_{\text{Ta}}} \frac{1}{M_{\text{Ta}}} \sum_{i=1}^{M_{\text{Ta}}} \left( f_{\text{Ta}}(X_{\text{Ta}}^{(i)}; \theta_0, \theta_{\text{Ta}}) - L_{\text{Ta}}^{(i)} \right)^2.
        \end{equation}
        
        \STATE \textbf{Output:} Final adapted \gls{lstm} metamodel $f_{\text{Ta}}(\cdot; \theta_0, \theta_{\text{Ta}})$ for the new \gls{va} contracts in the target task.
    \end{algorithmic}
    \end{algorithm}

Algorithm~\ref{alg3:combined} combines the strengths of Multi-task learning, layer freezing, and fine-tuning to accelerate the training of \gls{lstm} metamodels. 

\begin{itemize}
    \item \textbf{Multi-task learning} allows the shared \gls{lstm} layers to pool information across related tasks, and it allows the metamodel to learn generalizable representations of temporal dependencies.
    \item \textbf{Layer freezing} ensures that shared features learned from the source tasks are retained when adapting to a new task, reducing the risk of overfitting to the target simulation data.
    \item \textbf{Fine-tuning} enables the task-specific layers to adapt quickly to the new task with a minimum amount of simulation cost, leveraging the pre-trained shared layers.

\end{itemize}
    
The combination of these techniques significantly reduces the computational cost of training \gls{lstm} metamodels for a new but related \gls{va} contract.

\section{Numerical Experiments} \label{sec3:experiments}

In this section, we evaluate the performance of the \gls{tl} framework for rapid adaptation of \gls{lstm} metamodels in dynamic hedging of \gls{va}s.
The low noise dataset generated by the standard nested simulation procedure in Section~\ref{sec2:numerical} is used to train the source \gls{lstm} metamodels.
We consider \gls{tl} to two types of target tasks: 
\begin{itemize}
    \item the same contract but different underlying asset model, and
    \item different contracts but the same asset model.
\end{itemize}  
Similar to Section~\ref{sec2:numerical}, we use the standard nested simulation procedure to generate the dataset for the source and target tasks.
Serveral \gls{va} datasets generated with the standard nested simulation procedures under geometric Brownian motion (\gls{gbm}) and regime-switching \gls{gbm} (\gls{rsgbm}) asset models.
\textcolor{red}{In this experiment, we use the same \gls{rsgbm} asset model as Chapter~\ref{chap:project2} to generate the dataset for the source and target tasks.}
\textcolor{red}{When following a static lapse model and a dynamic lapse model, the monthly lapse rate is given by Equation~\ref{eq:lapse_rate} and Equation~\ref{eq:dynamic_lapse_rate}, respectively.}
\textcolor{red}{The experiments are run on a machine with a AMD Ryzen 9 7900X processor with 32 GB of RAM and a Nvidia RTX $\num{3060}$ Ti GPU}

\begin{table}[ht!] 
    \centering
    \begin{tabular}{lcccc} 
    \toprule
    \textbf{Contract} & \textbf{Asset Model} & \textbf{Lapse} & \textbf{$M_{\text{So}}$}  & \textbf{$M_{\text{Ta}}$}\\
    \midrule
    \gls{gmmb} & \gls{rsgbm} & No lapse & $\num{50000}$ & N/A \\
    \gls{gmmb} & \gls{rsgbm} & Static lapse & $\num{50000}$ & $\num{2000}$ \\
    \gls{gmmb} & \gls{rsgbm} & Dynamic lapse & $\num{50000}$ & $\num{2000}$ \\
    \gls{gmwb} & \gls{rsgbm} & Dynamic lapse & N/A & $\num{2000}$ \\
    \bottomrule
    \end{tabular}
    \caption{\gls{va} Contracts for Transfer Learning Experiments}
    \label{tab3:contracts}
\end{table}

Table~\ref{tab3:contracts} lists the \gls{va} contracts used in the \gls{tl} experiments.
These contracts include \gls{gmmb} and \gls{gmwb} contracts with no lapse, static lapse, and dynamic lapse features.
These contracts include \gls{gmmb} and \gls{gmwb} contracts with no lapse, static lapse, and dynamic lapse features.
Before transferred to the target task, a \gls{lstm} metamodel is trained on a source dataset with $M_{\text{So}} = \num{50000}$ samples generated with $N_{\text{So}} = \num{100}$ inner replications\footnotemark.
\footnotetext{The GMMB contract on the \gls{gbm} asset model is an exception, where scenario-wise contract losses can be computed analytically.}
The pre-trained \gls{lstm} metamodel is then adapted to a target task that is different from the source task.
The training dataset for the target task has $M_{\text{Ta}} = \num{2000}$ samples generated with $N_{\text{Ta}} = \num{100}$ inner replications.
During the training on both the source and target tasks, $10\%$ of the data is split into a validation dataset to monitor the training process and prevent overfitting by using early stopping.
The complexity of the simulation schemes increases from the first task to the last task, and the \gls{lstm} metamodels need to adapt to the new conditions with a limited amount of training data on the target tasks.

All \gls{lstm} metamodels are evaluated based on their training history graphs for the target task, which plot training and validation MSE against the number of training epochs.
The training history graphs visualize the learning curves of the \gls{lstm} metamodels, which provide insights into stability and convergence behavior the metamodels.
We measure the generalization performance of the \gls{lstm} metamodels using the true MSE, which quantifies the accuracy of the \gls{lstm} metamodels in approximating the true inner simulation model of \gls{va} contracts.
The true MSE is computed by comparing the metamodel predictions with the true contract losses, which are approximated by the standard nested simulation procedure with $\num{100000}$ inner replications.
Hyperparameters and network architectures for the \gls{lstm} metamodels are kept the same as the \gls{lstm} metamodel in Section~\ref{sec2:numerical}. 
Due to a limited amount of computational resources, macro replications are not feasible for the \gls{tl} experiments.


\subsection{Learning Lapse Features}

\begin{figure}[ht!]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{./project3/figures/figure1a.png}
        \caption{Extensive Training on Target Task} 
        \label{subfig3-1:extensive}
    \end{subfigure}\hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{./project3/figures/figure1b.png}
        \caption{Without \gls{tl}}
        \label{subfig3-1:without}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{./project3/figures/figure1c.png}
        \caption{With Fine-tuning}
        \label{subfig3-1:fineTuning}
    \end{subfigure}\hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{./project3/figures/figure1d.png}
        \caption{With Layer Freezing}
        \label{subfig3-1:layerFreezing}
    \end{subfigure}
    \caption{Metamodel performance on \gls{rsgbm} \gls{gmmb} with static lapse}
    \label{fig3:figure1}
\end{figure}

We first examine the performance of fine-tuning and layer freezing in adapting \gls{lstm} metamodels to new \gls{va} contracts with lapse features.
Figure~\ref{fig3:figure1} compares the performance of \gls{lstm} metamodels on the target tasks with and without \gls{tl}.
Learning histories of the \gls{lstm} metamodels is shown for the target task of metamodeling \gls{gmmb} contract losses on the \gls{rsgbm} asset model with static lapse.
The source task is hedging a \gls{gmmb} contract on \gls{rsgbm} with no lapse, and the target task is hedging the same \gls{gmmb} contract but with a static lapse.
The simulation data for the target task is generated with the same nested simulation procedure as the source task, except for the lapse feature.
Figure~\ref{subfig3-1:extensive} demonstrates the performance of the \gls{lstm} metamodel trained extensively only on the target task by using $M_{\text{TA}} = 50,\!000$ samples, which serves as a benchmark for this study. 
The metamodel achieves a low validation \gls{mse} due to the availability of a large dataset.
In contrast, Figure~\ref{subfig3-1:without} presents the results of training directly on the target task without pre-training on the source task.
With only $M_{\text{TA}} = 2,\!000$ samples, the \gls{lstm} metamodel struggles to learn the temporal dependencies and patterns in the target task.
It leads to highly unstable training dynamics with a substantial amount of fluctuations in the validation \gls{mse}.
Learning without knowledge transfer leads poor generalization and extreme overfitting. 
Often, the training data is limited for a new \gls{va} contract, and such instability is particularly problematic for a quick adaptation of \gls{lstm} metamodels.
\gls{tl} techniques like fine-tuning and layer freezing can help mitigate these challenges.
Figure~\ref{subfig3-1:fineTuning} shows the results of fine-tuning a pre-trained metamodel on \gls{rsgbm} with no lapse.
Fine-tuning offers a noticeable improvement over training without \gls{tl} by reducing the instability in the validation \gls{mse}. 
However, despite this improvement, fine-tuning may not fully mitigate the challenges of training with a small dataset.
No negative trasfer is observed, but the fine-tuned metamodel struggles to achieve a low validation \gls{mse}.
Lowering the learning rate does not help the fine-tuned metamodel converge, and the validation \gls{mse} remains high after increasing the number of training epochs.
It indicates that fine-tuning may not be sufficient for a limited amount of training data.
Figure~\ref{subfig3-1:layerFreezing} presents the performance of layer freezing on the same target task.
Layer freezing offers a more stable training process compared to fine-tuning, with a lower validation error and reduced fluctuations during its training.
The \gls{lstm} layers are critical for capturing general feature representations that are transferable across tasks, and freezing these layers helps prevent overfitting to the target task.
In addition, the layer freezing approach tunes fewer neural network parameters than crude fine-tuning.
It allows the transferred metamodel to only focus on learning the lapse features without excessively adjusting the general temporal representations learned from the source task for the target task.
This reduction in trainable parameters also accelerates the convergence, and it leads to a more stable and efficient training on the target task.

\subsection{Transfer to VAs with a Dynamic Lapse}

\begin{figure}[ht!]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{./project3/figures/figure2a.png}
        \caption{Fine-tuning from No Lapse} 
        \label{subfig3-2:fromNolapse}
    \end{subfigure}\hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{./project3/figures/figure2b.png}
        \caption{Fine-tuning from Static Lapse}
        \label{subfig3-2:fromLapse}
    \end{subfigure}
    \caption{Fine-tuned Metamodel performance on \gls{rsgbm} \gls{gmmb} with a dynamic lapse}
    \label{fig3:figure2}
\end{figure}

We further investigate the performance of fine-tuning on the target task of metamodeling \gls{gmmb} contract losses on \gls{rsgbm} with a dynamic lapse. 
The source tasks used for pre-training are \gls{gmmb} contracts on \gls{rsgbm} with no lapse and a static lapse, respectively. 
Figure~\ref{fig3:figure2} displays the learning curves of the \gls{lstm} metamodels fine-tuned from these two distinct source tasks.
We observe that the performance of the fine-tuned metamodel is highly dependent on the similarity between the source and target tasks.
Fine-tuning from a source task with a static lapse results in a faster convergence and a lower validation error.
The metamodel trained on the \gls{gmmb} with a static lapse captures some features that are beneficial for the \gls{gmmb} with a dynamic lapse, which leads to a more stable training process.
In Figure~\ref{subfig3-2:fromNolapse}, the metamodel needs to learn the effect of 

\begin{itemize}
    \item whether lapse is present, and
    \item whether the lapse is dynamic.
\end{itemize}
This makes learning more challenging.

The observed improvement in transferability when fine-tuning from a static lapse source task highlights the importance of selecting appropriate source tasks in \gls{tl}. 
When the source task diverges substantially from the target task, the transferred metamodel may struggle to adapt to the new conditions given the limited amount of training data.


\begin{figure}[ht!]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{./project3/figures/figure3a.png}
        \caption{Freezing LSTM Layers} 
        \label{subfig3-3:freezeLSTM}
    \end{subfigure}\hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{./project3/figures/figure3b.png}
        \caption{Freezing the Fully Connected Layer}
        \label{subfig3-3:freezeFC}
    \end{subfigure}
    \caption{Layer Freezing on \gls{rsgbm} \gls{gmmb} with dynamic lapse}
    \label{fig3:figure3}
\end{figure}

Figure~\ref{fig3:figure3} continues the investigation of layer freezing in the task of metamodeling \gls{gmmb} contract losses on \gls{rsgbm} with the dynamic lapse. 
When transferring knowledge from the \gls{gmmb} model with the static lapse, the primary adaptation for the target task involves learning the impact of the dynamic lapse.
Freezing the \gls{lstm} layers and fine-tuning only the fully connected layer results in a higher validation error, which suggests a tendency of overfitting.
This indicates that the fully connected layer struggles to adapt to the changes in the temporal dynamics introduced by a dynamic lapse, which can be viewed as another source of randomness in the time series.
In contrast, freezing the fully connected layer and fine-tuning the \gls{lstm} layers leads to a lower validation error and better generalization. 
This can be attributed to the fact that the \gls{lstm} layers are responsible for capturing the temporal dependencies associated with the dynamic lapse, and the fully connected layer predicts the contract losses based on these learned features.

This experiment emphasizes the importance of choosing which layers to freeze based on the nature of the source and target tasks. 
In the case of learning the dynamic lapse features, freezing the \gls{lstm} layers is not beneficial as they need to adapt to the new temporal patterns.

\begin{table}[ht!]
    \centering
    \begin{tabular}{lllll}
    \toprule
    \textbf{Lapse Type} & \textbf{Extensive} & \textbf{Fine-tuning} & \textbf{Layer Freezing} & \textbf{Without \gls{tl}} \\
    \midrule
    No Lapse & N/A & $0.4894$ & $0.3361$ & N/A \\
    Static Lapse & N/A & $0.0794$ & $0.0763$ & N/A \\
    Dynamic Lapse & $0.0587$ &  N/A &  N/A & $0.2950$ \\
    \bottomrule
    \end{tabular}
    \caption{Comparison of different \gls{tl} methods on \gls{gmmb} contracts}
    \label{tab3:transfer_learning_results}
\end{table}

Table~\ref{tab3:transfer_learning_results} summarizes the true \gls{mse}s of the \gls{lstm} metamodels trained using different \gls{tl} methods across various source tasks.
The calculations are based on the metamodel predictions and the true contract losses approximated with $\num{100000}$ inner replications.
The first two rows show the performance of transferring knowledge from \gls{gmmb} contracts with no lapse and the static lapse to the target task of \gls{gmmb} with the dynamic lapse.
The last row shows the performance of training without \gls{tl} on the target task.
The results demonstrate the effectiveness of fine-tuning and layer freezing in transferring knowledge from a related source tasks to the target task. 
When transferring from a source task with the static lapse to a target task with the dynamic lapse, the MSEs achieved are $0.0794$ for fine-tuning and $0.0763$ for layer freezing.
The benchmark MSE of $0.0587$, obtained from extensive training on the \gls{gmmb} dynamic lapse task with much more samples.

While \gls{tl} from static lapse \gls{gmmb} do not reach this level of accuracy due to the limited data in the target task, they substantially outperform training without \gls{tl}. 
This indicates that the models pre-trained on a static lapse setting are effective in capturing relevant features that are transferable to the dynamic lapse scenario.

However, when the source task is less similar to the target task, the benefits of \gls{tl} are less pronounced. The MSEs in this case are higher, with fine-tuning resulting in $0.4894$ and layer freezing achieving $0.3361$. 
This suggests that the divergence between the source and target tasks can lead to negative transfer.
These results highlight the importance of selecting source tasks that share significant similarities with the target task to maximize the effectiveness of \gls{tl}. 
When the source and target contracts are substantially different, the pre-trained metamodels may struggle to adapt to the new conditions, as they may not have learned features relevant to the target task.
There is no negative transfer observed, but the knowledge transfer is limited.

\subsection{Transfer Knowledge to other Contract Types}

When transferring knowledge from one \gls{va} contract type to another, the \gls{lstm} metamodel needs to adapt to different contract features and time series dynamics.
We consider the task of metamodeling \gls{gmwb} contract losses on \gls{rsgbm} asset model with the dynamic lapse, with the source tasks being a \gls{gmmb} contract on \gls{rsgbm} also with the dynamic lapse.
Figure~\ref{fig3:figure4} illustrates the learning history of transferring pre-trained \gls{lstm} metamodels from the \gls{gmmb} contracts to the \gls{gmwb} contracts.


\begin{figure}[ht!]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{./project3/figures/figure4a.png}
        \caption{Extensive Training on Target Task} 
        \label{subfig3-4:extensive}
    \end{subfigure}\hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{./project3/figures/figure4b.png}
        \caption{Without \gls{tl}}
        \label{subfig3-4:without}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{./project3/figures/figure4c.png}
        \caption{With Fine-tuning}
        \label{subfig3-4:fineTuning}
    \end{subfigure}\hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{./project3/figures/figure4d.png}
        \caption{With Layer Freezing}
        \label{subfig3-4:layerFreezing}
    \end{subfigure}
    \caption{TL performance on RS-GBM GMWB with dynamic lapse}
    \label{fig3:figure4}
\end{figure}

Figure~\ref{subfig3-4:extensive} presents the performance of an extensively trained \gls{lstm} metamodel on the target task using $\num{50000}$ samples, serving as a benchmark of the best metamodel performance. 
The extensively trained metamodel achieves a low validation error and demonstrates stable convergence due to having enough training samples. 
In contrast, Figure~\ref{subfig3-4:without} shows the results of training the metamodel directly on the target task with $\num{2000}$ training samples. 
Fewer training samples and no prior knowledge leads to unstable training dynamics. 
The validation MSE fluctuates substantially.

When applying fine-tuning from the \gls{gmmb} source task (Figure~\ref{subfig3-4:fineTuning}), the metamodel exhibits an improved performance compared to the training without \gls{tl}. 
Fine-tuning results in a lower validation error and more stable convergence.
Despite the differences between the \gls{gmmb} and \gls{gmwb} contracts, there is still valuable information that can be transferred. 
Both the \gls{lstm} layers and the fully connected layers capture general temporal patterns and feature representations that are beneficial for the target task. 
Fine-tuning allows the metamodel to adjust all its neural network layers. 
It allows a better adaptation to the complexities introduced by the \gls{gmwb} contract.

However, when employing layer freezing (Figure~\ref{subfig3-4:layerFreezing}), the metamodel's performance deteriorates.
Negative transfer is observed.
Freezing some layers trained on the \gls{gmmb} contracts does not allow the metamodel to sufficiently adapt to the complexities of the \gls{gmwb} contracts. 
The \gls{gmwb} contracts are inherently more complex than \gls{gmmb} contracts due to the guaranteed withdrawal benefits at each time step, and the contract features are substantially different.
The complexity introduced by the \gls{gmwb} contracts leads to significant changes in the time series dynamics that the metamodel needs to capture.
Freezing the \gls{lstm} layers or the fully connected layer hinders the metamodel's ability to learn these new patterns, and it leads to a poor generalization and unstable error curves.

\begin{table}[ht!]
    \centering
    \begin{tabular}{lrrr}
        \toprule
        \textbf{Model} & \textbf{Training samples} & \textbf{Training MSE} & \textbf{True MSE} \\
        \midrule
        Without \gls{tl} & $\num{2000}$ & $0.3588$ & $0.4188$ \\
        Fine-tuning & $\num{2000}$ & $0.1690$ & $0.1780$ \\
        Layer freezing & $\num{2000}$ & $0.1828$ & $0.2295$ \\
        Extensive training & $\num{50000}$ & $0.0853$ & $0.0726$ \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of different TL methods to GMWB contracts}
    \label{tab3:transfer_learning_results_gmwb}
\end{table}

Table~\ref{tab3:transfer_learning_results_gmwb} summarizes the true MSEs of the \gls{lstm} metamodels on the \gls{gmwb} contracts.
Training MSE and true MSE are computed using the training labels and the true contract losses approximated with $\num{100000}$ inner replications, respectively.
This is consistent with the terminology used in Chapter~\ref{chap:project2}.
The \gls{tl} methods are compared to training from the scratch.
The suboptimal performance of layer freezing in this context indicates that the difference between the source and target tasks is too substantial for this method to be effective. 
While layer freezing can be advantageous when the source and target tasks are closely related, it may hinder performance when the tasks diverge substantially.
In such circumstances, fine-tuning provides a better approach by allowing the metamodel to leverage transferable knowledge while adapting to the new task's specific requirements. 
Fine-tuning enables both the \gls{lstm} and the fully connected layers to update their weights, capturing the complex dynamics of the \gls{gmwb} contracts more effectively.
This is particularly beneficial when developing metamodels for the complex \gls{va} contracts with limited simulation data. 
For instance, transferring information from the \gls{gmmb} contracts can still be valuable when modeling the \gls{gmwb} contracts.
Both contracts share some common contract features and temporal patterns, which allows a pre-trained \gls{lstm} metamodel to capture generalizable features that provide a solid foundation for the target task.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{./project3/figures/figure4_1.png}
        \caption{Identifying Tail scenarios} 
        \label{subfig3-4-1:tail}
    \end{subfigure}\hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{./project3/figures/figure4_2.png}
        \caption{$95\%$-CVaR predictions}
        \label{subfig3-4-2:CVaR}
    \end{subfigure}
    \caption{TL performance on \gls{rsgbm} GMWB with dynamic lapse}
    \label{fig3:figure4-1}
\end{figure}

Figure~\ref{fig3:figure4-1} illustrates the performance of the two-stage procedure with \gls{tl} for \gls{gmwb} contracts with the dynamic lapse in predicting the tail scenarios and the $95\%$-CVaR.
The graph provides a visual representation of how different \gls{tl} approaches compare to training from the scratch and the standard nested simulation procedure.
Fine-tuning consistently outperforms both layer freezing and training from the scratch across different simulation budgets, particularly in predicting tail scenarios and estimating the 95\%-CVaR. 

This finding is consistent with the training history and MSEs in Figure~\ref{fig3:figure4} and Table~\ref{tab3:transfer_learning_results_gmwb}, respectively.
It is important to note that while these results are promising, they also highlight the complexity of modeling the \gls{gmwb} contracts with the dynamic lapse. 
The fact that fine-tuning outperforms layer freezing suggests that there are substantial differences in the tail behavior of the \gls{gmmb} and \gls{gmwb} contracts, particularly when the dynamic lapse is considered. 
These findings highlight the need for careful model selection and validation when applying the \gls{tl} techniques to different \gls{va} products.
When the source and target tasks are substantially different, all layers need to be updated.
Freezing some layers may lead to negative transfer.


\subsection{Multi-task Learning}

Multi-task learning enables the \gls{lstm} metamodels to learn shared representations across related \gls{va} contracts.
In this section, we examine the performance of multi-task learning applied to two types of \gls{va}s, \gls{gmmb} and \gls{gmwb} with the dynamic lapse rates.
The simulation datasets contain $2,\!000$ samples for each contract type, and the \gls{lstm} metamodels are trained using multi-task learning. 
In our experiments, Algorithm~\ref{alg3:multiTaskLearning} is used to train the \gls{lstm} metamodels simultaneously to minimize the multi-task MSE loss function in Equation~\eqref{eq3:multiTaskLoss}.
We use individual task training as a baseline for comparison, where the \gls{lstm} metamodels are trained separately on the \gls{gmmb} and \gls{gmwb} contracts.
The objective is to assess how multi-task learning can improve the training efficiency and performance of both products compared to individual task training.

\begin{figure}[ht!]
    \includegraphics[width=\textwidth]{./project3/tikz/mtl.pdf}
    \caption{Multi-task learning framework for VA contracts}
    \label{fig3:mtl}
\end{figure}

Figure~\ref{fig3:mtl} illustrates our multi-task learning framework for \gls{gmmb} and \gls{gmwb} contracts with the dynamic lapse rates.
The outer simulation paths for both contracts are the same, which are generated from the same nested simulation procedure with $100$ inner replications.
The \gls{lstm} metamodels are trained simultaneously on both contracts.
The \gls{lstm} layers are shared to train for capturing general temporal patterns, and each contract has its own fully connected layers that are trained for contract loss predictions.


\begin{figure}[ht!]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{./project3/figures/figure5a.png}
        \caption{Multi-task training history} 
        \label{subfig3-5:multiTask}
    \end{subfigure}\hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{./project3/figures/figure5b.png}
        \caption{Task performance with multi-task training}
        \label{subfig3-5:fineTuning}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{./project3/figures/figure5c.png}
        \caption{GMMB individual task training} 
    \label{subfig3-5:gmmb_individual}
    \end{subfigure}\hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{./project3/figures/figure5d.png}
        \caption{GMWB individual task training}
        \label{subfig3-5:gmwb_individual}
    \end{subfigure}
    \caption{Multi-task Learning on \gls{rsgbm} GMMB and GMWB with dynamic lapse}
    \label{fig3:figure5}
\end{figure}

Figure~\ref{fig3:figure5} compares the learning curves for the training of the \gls{gmmb} and the \gls{gmwb}, with and without the multi-task learning.
The comparison between the mult-itask learning (Figures~\ref{subfig3-5:multiTask} and~\ref{subfig3-5:fineTuning}) and the individual task training ((Figures~\ref{subfig3-5:gmmb_individual} and~\ref{subfig3-5:gmwb_individual})) demonstrates the benefit of the multi-task learning for both products. 

In the case of the \gls{gmmb}, the multi-task learning allows the model to achieve faster convergence while reducing overfitting.
For the \gls{gmwb}, the multi-task learning framework helps to stablize the training process.
The shared \gls{lstm} layers in the multi-task model are able to capture common temporal patterns and dynamics, which benefits both the \gls{gmmb} and the \gls{gmwb} when the training samples are scarce.

Similar to the pooling in Chapters~\ref{chap:project1} and~\ref{chap:project2} but on a higher level, multi-task learning enables the \gls{lstm} metamodels to leverage shared representations across related \gls{va} contracts.

% \begin{figure}[ht!]
%     \centering
%     \begin{subfigure}{0.48\textwidth}
%         \includegraphics[width=\textwidth]{./project3/figures/figure5a.png}
%         \caption{Multi-task training history} 
%         \label{subfig3-6:multiTask}
%     \end{subfigure}\hfill
%     \begin{subfigure}{0.48\textwidth}
%         \includegraphics[width=\textwidth]{./project3/figures/figure5b.png}
%         \caption{Task performance with multi-task training}
%         \label{subfig3-6:fineTuning}
%     \end{subfigure}    
%     \label{fig3:figure6}
% \end{figure}

% Even when data is abundant, multi-task learning can improve the training efficiency and predictive performance of \gls{lstm} metamodels for nested simulations.
% The shared LSTM layers learn generalizable features that are beneficial across different \gls{va} contracts.
% Figure~\ref{fig3:figure6} demonstrates the performance of multi-task learning on GMMB and GMWB contracts with dynamic lapse rates.

\section{Conclusion} \label{sec3:conclusion}

In this chapter, we have introduced a \gls{tl} framework to accelerate the training of \gls{lstm} metamodels for dynamic hedging of variable annuity contracts.
Traditional nested simulation procedures for \gls{va} risk management are computationally intensive.
\gls{lstm} metamodels offer a data-driven approach to approximate the true contract losses, which can substantially reduce the computational cost of hedging a single \gls{va} contract.
However, training \gls{lstm} metamodels on new \gls{va} contracts can be challenging due to the limited availability of simulation data.
Our proposed framework leverages pre-trained \gls{lstm} networks and \gls{tl} techniques to adapt metamodels quickly to new but related \gls{va} contracts with a minimum amount of additional simulation cost.
Fine-tuning a pre-trained \gls{lstm} metamodel on a new target task with a limited amount of data substantially improved training stability and predictive accuracy compared to training from the scratch. 

Layer freezing further enhanced performance by retaining transferable temporal representations learned from the source task. 
However, the success of these methods depends on the similarity between the source and target tasks. 
When the tasks were closely related, freezing some neural network layers yields substantial benefits. 
Conversely, when the source and target tasks diverged substantially, fine-tuning the entire network is more effective.

Multi-task learning offers a robust framework for transferring knowledge across related simulation schemes in the \gls{lstm} metamodeling for nested simulations. 
By sharing representations through the \gls{lstm} layers, the model can learn more generalizable features that are beneficial across different \gls{va} contracts. 
The multi-task approach is particularly beneficial when training data for individual tasks is scarce, as it effectively pools information across tasks to enhance learning efficiency and predictive performance.
We have extended the idea of pooling in Chapter~\ref{chap:project1} and~\ref{chap:project2} to a higher level.
Furthermore, a metamodel trained with multi-task learning can serve as a pre-trained model. 
It is adaptable to various \gls{va} contracts and asset models, and it is a versatile tool for practical applications in dynamic hedging of \gls{va}s.

The integration of \gls{tl} in \gls{lstm} metamodeling represents an important advancement in robust risk management associated with \gls{va} contracts.
By effectively leveraging existing knowledge, financial institutions can maintain accurate and responsive risk management practices in a rapidly changing market environment. 
The methodologies presented in this chapter attempt to make a contribution to the broader applications of \gls{tl} in financial modeling and risk assessment.

