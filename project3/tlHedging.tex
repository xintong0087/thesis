\chapter{Model-Free Deep Hedging of Variable Annuities}

\section{Introduction}

Reinforcement Learning (RL) is a branch of machine learning that focuses on training algorithms, known as agents, to make a sequence of decisions. 
The agent learns to achieve a goal in an uncertain, potentially complex environment by trial and error, using feedback from its own actions and experiences. 
Unlike supervised learning, where training data is labeled with the correct answers, in RL, an agent is provided with rewards or punishments as signals for its actions.

The core of RL revolves around the concept of the agent interacting with its environment over time, aiming to maximize the cumulative reward. 
This process involves observing the state of the environment, selecting and performing actions, and receiving rewards or penalties in response to those actions. 
The agent's objective is to learn a mapping from states to actions that maximizes this cumulative reward, which is often referred to as a policy. 
One of the fundamental frameworks for modeling RL problems is the Markov Decision Process (MDP). 
An MDP provides a mathematical formulation of the decision-making process, characterized by states, actions, rewards, and transition probabilities. Solving an MDP involves finding a policy that maximizes some function of the expected rewards, typically the expected cumulative reward over time.

Reinforcement learning algorithms can be broadly categorized into two types: model-based and model-free approaches. 
Model-based methods utilize a model of the environment to simulate the outcomes of actions, enabling planning and decision-making with fewer interactions with the environment. 
Conversely, model-free methods learn directly from interactions with the environment, without relying on a model, making them more straightforward but often less efficient in terms of sample usage.

A compelling application of RL is in the hedging of financial derivatives, a domain where the complexity and uncertainty of financial markets make traditional static models inadequate.
Financial derivatives are contracts whose value is derived from an underlying asset, and hedging is the practice of making investments to reduce the risk of adverse price movements.
RL's adaptability and learning capabilities offer a promising solution to dynamically adjust hedging strategies in response to market movements.
This approach, particularly with model-free algorithms, has the potential to enhance risk management practices by developing strategies that can adapt in real-time to changing market conditions.

In this paper, we propose a model-free RL algorithm to improve the risk management of variable annuities (VAs) by learning a hedging strategy from past experience.
VAs are popular retirement products that combine investment and insurance features, offering policyholders the potential for investment growth and the protection of a guaranteed minimum death benefit or income benefit.
Due to the complexity of the products and the dynamic nature of the financial markets, the task of hedging VAs is particularly challenging.
Our model-free RL approach aims to learn an optimal hedging policy for VAs without relying on a predefined model of the underlying asset dynamics.
By leveraging the flexibility and adaptability of RL algorithms, we aim to develop a dynamic hedging strategy that can effectively manage the risks associated with VAs in a changing market environment.

The rest of the paper is organized as follows: Section~\ref{sec3:vaHedging} presents the problem formulation for hedging variable annuity with a deep neural network, 

\section{Dynamic Hedging of Variable Annuities} \label{sec3:vaHedging}


\subsection{Deep Hedging Approaches}

Deep hedging is a reinforcement learning approach that leverages deep learning models to optimize hedging strategies for financial derivatives.
In deep hedging, the objective is to minimize the hedging error or a risk measure over a set of paths of the underlying asset, which involves training the DNNs using backpropagation and optimization algorithms to find the hedging strategy that maximizing the value function.

\begin{enumerate}
    \item \textbf{Value Function:} Specify a value function that quantifies the hedging performance. 
    This could include minimizing the expected liability, the CVaR, or the variance of portfolio returns.
    \item \textbf{Training and Optimization:} Using historical data or simulated data generated from the specified market model, train the DNN using backpropagation and optimization algorithms to find the hedging strategy that maximizes the value function.
    \item \textbf{Implementation and Adjustment:} Implement the learned hedging strategy in real-time trading, with periodic adjustments based on new market information and continuous learning to adapt to changing market dynamics. This is often referred to as online learning.
\end{enumerate}

Depending on the specific market model and the hedging objective, deep hedging can be categorized into model-based and model-free approaches.
Model-based deep hedging trains a deep neural network (DNN) to learn optimal hedging actions by maximizing a value function that reflects the hedging error or risk measure, such as the variance of the hedging portfolio's final value or tail risk measures like the CVaR.
The DNN processes sequential market data and outputs a policy that determines the hedging actions based on the current state of the market.
In the context of deep hedging, the policy $\pi = \phi(\mathcal{I})$ is defined by the parameter $\phi$ of a DNN, and the information $\mathcal{I}$ is the training data during the learning process.
~\cite{buehler2019deep} is the first paper to propose a model-based deep hedging approach for hedging financial derivatives with a focus on minimizing CVaR.
~\cite{carbonneau2021deep} extends the model-based approach to hedging long-term financial derivatives and compares the performance of deep hedging strategies trained with different reward functions.
A more recent paper by~\cite{imaki2021no} proposes a new network architecture for model-based deep hedging that can be trained more efficiently by implementing a no-tranaction band.
A typical setup for a model-based deep hedging strategy includes:

\begin{enumerate}[label=\arabic*a.]
    \setcounter{enumi}{3}
    \item \textbf{Market Model Specification:} Define the underlying asset price dynamics and the risk factors affecting the financial instruments. Common models include geometric Brownian motion for stock prices or more complex models accounting for jumps or stochastic volatility.~\cite{glasserman2004monte} provides detailed discussions on simulating asset prices under various models.
    \item \textbf{Deep Neural Network Design:} Design a DNN architecture capable of processing sequential market data and making hedging decisions. The network outputs a policy by receiving inputs that include historical prices, option strikes, and other relevant financial indicators.~\cite{buehler2019deep} uses a feedforward neural network (FNN), where its input layer receives the current state of the market, and its output layer produces the hedging weights.
\end{enumerate}

In summary, model-based deep hedging approaches rely on explicit market models to learn the optimal hedging strategies.
It requires the knowing of the underlying asset price dynamics and the risk factors affecting the financial instruments, which may limit its flexibility and robustness to model misspecification.
~\cite{chong2023pseudo} specifically discuss the effect of model misspecification on the performance of model-based deep hedging strategies for hedging variable annuities with a guaranteed minimum death benefit (GMDB) rider under the Black-Scholes framework.

Conversely, model-free deep hedging approaches directly learn from historical market data without making explicit assumptions about asset price dynamics. 
Instead, the value functions are learned directly from the data, and the hedging strategies are derived from the learned value functions.
A typical setup for a model-free deep hedging strategy includes:

\begin{enumerate}[label=\arabic*b.]
    \setcounter{enumi}{3}
    \item \textbf{Market Model Specification:} No explicit model assumptions are made about the underlying asset price dynamics. Instead, the deep learning model learns the value function and hedging strategies directly from historical market or simulation data.
    \item \textbf{Deep Neural Network Design:} Design two DNNs: a value network and a policy network. The value network learns the value function, while the policy network learns the optimal hedging actions based on the approximated value function.
\end{enumerate}

A model-free approach does not require explicit model assumptions about asset price dynamics, making it more flexible and adaptable to different market conditions and more robust to model misspecification.

\subsection{Value-based and Policy-based Deep Reinforcement Learning}

Model-free deep reinforcement learning algorithms can be broadly categorized into two types: value-based and policy-based approaches.

\begin{enumerate}
    \item \textbf{Value-based Deep Reinforcement Learning:} Value-based approaches learn the value function directly from the data and derive the policy from the learned value function. 
    The value function is learned by training a DNN to approximate the value function, which quantifies the expected cumulative reward over time. 
    The policy is then derived from the learned value function by selecting the action that maximizes the value function at each state. 
    ~\cite{mnih2015human} is the first paper to demonstrate the effectiveness of value-based deep reinforcement learning in training agents to play Atari games.
    To update the value function, the DNN is trained to minimize the difference between the current value function and the target value function, which is updated based on the maximum expected reward from the previous iteration.
    When trained to approximate the value function, the DNN benefits from the use of experience replay buffers, which is proposed by~\cite{lin1992self} to store and sample historical transitions and uniformly sampled during training to enhance the sample efficiency and stability of the learning process.
    ~\cite{kolm2019dynamic} uses a value-based deep reinforcement learning approach to hedge European options under the Black-Scholes framework.

    \item \textbf{Policy-based Deep Reinforcement Learning:}
    In valued-based approaches, the value function is updated iteratively with the maximum expected reward of all possible actions, which can be computationally expensive for continuous action spaces.
    To overcome this difficulty, a policy-based approach estimates the value function before realization of the final payoff of the hedging portfolio, which is particularly useful for hedging long-term financial derivatives or VA contracts.
    Two types of policy-based approaches prevail in the literature: actor-critic and policy gradient methods.

    An actor-critic method designs an actor network and a critic network to learn the value function and the policy simultaneously.
    The actor network learns the policy by directly outputting the action based on the current state, while the critic network learns the value of an action by approximating the expected cumulative reward over time following the actor's policy.
    A popular actor-critic algorithm is the Deep Deterministic Policy Gradient (DDPG) algorithm proposed by~\cite{lillicrap2015continuous}, which is implemented by~\cite{xu2022delta} in hedging financial derivatives in S\&P 500 and DJIA index options.
    
    A policy gradient method uses policy gradient theorem to update the policy by performing gradient ascent with respect to the policy parameters.
    Representative algorithms include the Trust Region Policy Optimization (TRPO) algorithm proposed by~\cite{schulman2015trust} and the Proximal Policy Optimization (PPO) algorithm proposed by~\cite{schulman2017proximal}.

    The most relevant paper to our study is~\cite{chong2023pseudo}, which uses the PPO algorithm to hedge GMMB with a GMDB rider under the Black-Scholes framework.
    However,~\cite{chong2023pseudo} implement the PPO algorithm with the knowing of the underlying asset model and the risk factors affecting the mortality rate, while our study focuses on the model-free deep reinforcement learning approach to hedge VAs without explicit model assumptions.

\end{enumerate}

\section{Proximal Policy Optimization for Hedging Variable Annuities}

PPO is a policy gradient method that aims to optimize the policy by performing gradient ascent with respect to the policy parameters.
Originally proposed by~\cite{schulman2017proximal} and inspired by TRPO, PPO aims to address the limitation of a crude policy gradient method by introducing a clipped surrogate objective function that prevents large policy updates and ensures more stable training.
While TRPO uses a hard coded constraint on the policy update to ensure that the new policy does not deviate too far from the old policy, PPO uses a clipped surrogate objective function that penalizes a high Kullback–Leibler (KL) divergence between the new and old policies. 
This allows for more significant policy updates while maintaining the stability of the training process.

The PPO algorithm consists of two main components: the policy network and the value network.
The policy network learns the policy by outputting the action based on the current state, while the value network learns the value function by approximating the expected cumulative reward over time following the policy.
The policy network is updated by performing gradient ascent with respect to the policy parameters, while the value network is updated by minimizing the mean squared error between the predicted value and the target value.

The PPO algorithm is particularly well-suited for continuous action spaces and has been successfully applied to a wide range of reinforcement learning tasks, including robotic control, game playing, and financial trading.
In the context of hedging VAs, PPO can be used to learn an optimal hedging policy by maximizing a reward function that reflects the hedging performance.


\subsection{Markov Decision Process (MDP) for Hedging VAs}
A Markov Decision Process (MDP) provides a mathematical framework for solving sequential decision-making tasks in situations where outcomes are partly random and partly under the control of an agent. 
In the context of hedging VAs, an MDP can be used to model the decision-making process of an insurer who aims to minimize the liability of the VA by dynamically adjusting the hedging portfolio based on the current state of the VA and the financial markets.
In this section, we reformulate the hedging problem of VAs from Section~\ref{subsec:VApayout} as an MDP and define the key components of the MDP.
An MDP is defined by the tuple $\mathcal{M} = (\mu_0, \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$, where:

\begin{itemize}
    \item $\mu_0$ is the initial states.
    \item $\mathcal{S}$ is a finite set of states.
    \item $\mathcal{A}$ is a finite set of actions.
    \item $\mathcal{P}$ is a state transition probability distribution, where $\mathcal{P}(s'|s, a)$ represents the probability of transitioning from state $s$ to state $s'$ due to action $a$.
    \item $\mathcal{R}$ is a reward distribution, $\mathcal{R}(s, a, s')$ is the reward received after transitioning from state $s$ to state $s'$ due to action $a$.
    \item $\gamma$ is a discount factor, $\gamma \in [0,1]$,  which models the present value of future rewards.
\end{itemize}

The objective in an MDP is to find a policy, $\pi: \mathcal{S} \rightarrow \mathcal{A}$, that maximizes the cumulative reward over time, often referred to as the value function for a policy $\pi$ at state $s$: 

\begin{equation} \label{eq3:V_pi}
    V^{\pi}_{\mathcal{M}}(s) = \mathbb{E}_{s_0 \sim \mu_0, a_i \sim \pi(\cdot|s_i), s_{i+1} \sim \mathcal{P}(\cdot|s, a)} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} |\pi, s \right],
\end{equation}

where $R_{t+1} = \mathcal{R}(s_i, a_i, s_{i+1})$ is the reward received by the agent after taking action $a_i$ in state $s_i$ and transitioning to state $s_{i+1}$, and the expectation is taken over the possible sequences of states and rewards that follow from the policy $\pi$.

In the context of hedging VAs, the initial states $\mu_0$ specifies the initial value of the VA and the market specifications. 
The state space $\mathcal{S}$ represent the information available to the agent, such as current value of the underlying asset ($S_t$), the current subaccount value ($F_t$), and the guarantee value ($G_t$), as well as other relevant contract specifications.
The action space $\mathcal{A}$ represent the hedging decisions, such as the hedging portfolio weights.
The state transition probability distribution $\mathcal{P}$ and the reward function $\mathcal{R}$ are derived from the dynamics of the VA and the market specifications. Instead of having an infinite time horizon, the final reward is obtained at the maturity of the VA contract. Hence, $R_{t} = 0$ for all $t > T$.
The discount factor $\gamma$ models the present value of future rewards and is conveniently inherited from the risk-free rate. 

Considering the task of hedging a GMWB contract, the hedging strategy aims to minimize the insurer's liability $L_t$ at each time $t=1,\ldots,T$ by dynamically adjusting the hedging portfolio based on the current state of the VA and the financial markets. The objective is to find a policy $\pi$ that minimizes the expected liability over time, i.e., 

\begin{equation}
    \min_{\pi} \mathbb{E}[\sum_{t=1}^{T} \gamma^t L_t | \pi, s_0],
\end{equation}

where it can be conveniently rewritten as the value function $V^{\pi}_{\mathcal{M}}(s)$ in Equation~\ref{eq3:V_pi} by setting the reward function $R_{t} = -L_t$ and the discount factor $\gamma = \frac{1}{1+r}$. 
Other value functions can be used to optimize over a risk measure on the cumulative liability, such as the Value-at-Risk (VaR) or the Conditional Value-at-Risk (CVaR).
Denote the risk measure as $\rho_{\alpha}$, where $\alpha$ is the confidence level, then the value function can be set as

\begin{equation}
    V^{\pi}_{\mathcal{M}}(s) = \rho_{\alpha}(\sum_{t=1}^{T} \gamma^t L_t | \pi, s_0).
\end{equation}