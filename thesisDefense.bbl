\begin{thebibliography}{17}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andreas et~al.(2017)Andreas, Klein, and Levine]{andreas2017modular}
Jacob Andreas, Dan Klein, and Sergey Levine.
\newblock Modular multitask reinforcement learning with policy sketches.
\newblock In \emph{International conference on machine learning}, pages
  166--175. PMLR, 2017.

\bibitem[Buehler et~al.(2019)Buehler, Gonon, Teichmann, and
  Wood]{buehler2019deep}
Hans Buehler, Lukas Gonon, Josef Teichmann, and Ben Wood.
\newblock Deep hedging.
\newblock \emph{Quantitative Finance}, 19\penalty0 (8):\penalty0 1271--1291,
  2019.

\bibitem[Chong et~al.(2023)Chong, Cui, and Li]{chong2023pseudo}
Wing~Fung Chong, Haoen Cui, and Yuxuan Li.
\newblock Pseudo-model-free hedging for variable annuities via deep
  reinforcement learning.
\newblock \emph{Annals of Actuarial Science}, 17\penalty0 (3):\penalty0
  503--546, 2023.

\bibitem[Harutyunyan et~al.(2015)Harutyunyan, Devlin, Vrancx, and
  Now{\'e}]{harutyunyan2015expressing}
Anna Harutyunyan, Sam Devlin, Peter Vrancx, and Ann Now{\'e}.
\newblock Expressing arbitrary reward functions as potential-based advice.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~29, 2015.

\bibitem[Imaki et~al.(2021)Imaki, Imajo, Ito, Minami, and
  Nakagawa]{imaki2021no}
Shota Imaki, Kentaro Imajo, Katsuya Ito, Kentaro Minami, and Kei Nakagawa.
\newblock No-transaction band network: A neural network architecture for
  efficient deep hedging.
\newblock \emph{arXiv preprint arXiv:2103.01775}, 2021.

\bibitem[Kolm and Ritter(2019)]{kolm2019dynamic}
Petter~N Kolm and Gordon Ritter.
\newblock Dynamic replication and hedging: A reinforcement learning approach.
\newblock \emph{The Journal of Financial Data Science}, 1\penalty0
  (1):\penalty0 159--171, 2019.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Lin(1992)]{lin1992self}
Long-Ji Lin.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Machine learning}, 8:\penalty0 293--321, 1992.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999policy}
Andrew~Y Ng, Daishi Harada, and Stuart Russell.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In \emph{Icml}, volume~99, pages 278--287. Citeseer, 1999.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pages
  1889--1897. PMLR, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Teh et~al.(2017)Teh, Bapst, Czarnecki, Quan, Kirkpatrick, Hadsell,
  Heess, and Pascanu]{teh2017distral}
Yee Teh, Victor Bapst, Wojciech~M Czarnecki, John Quan, James Kirkpatrick, Raia
  Hadsell, Nicolas Heess, and Razvan Pascanu.
\newblock Distral: Robust multitask reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wiewiora et~al.(2003)Wiewiora, Cottrell, and
  Elkan]{wiewiora2003principled}
Eric Wiewiora, Garrison~W Cottrell, and Charles Elkan.
\newblock Principled methods for advising reinforcement learning agents.
\newblock In \emph{Proceedings of the 20th international conference on machine
  learning (ICML-03)}, pages 792--799, 2003.

\bibitem[Xiao et~al.(2021)Xiao, Yao, and Zhou]{xiao2021optimal}
Bo~Xiao, Wuguannan Yao, and Xiang Zhou.
\newblock Optimal option hedging with policy gradient.
\newblock In \emph{2021 International Conference on Data Mining Workshops
  (ICDMW)}, pages 1112--1119. IEEE, 2021.

\bibitem[Xu and Dai(2022)]{xu2022delta}
Wei Xu and Bing Dai.
\newblock Delta-gamma-like hedging with transaction cost under reinforcement
  learning technique.
\newblock \emph{The Journal of Derivatives}, 29\penalty0 (5):\penalty0 60--82,
  2022.

\bibitem[Zhang et~al.(2018)Zhang, Satija, and Pineau]{zhang2018decoupling}
Amy Zhang, Harsh Satija, and Joelle Pineau.
\newblock Decoupling dynamics and reward for transfer learning.
\newblock \emph{arXiv preprint arXiv:1804.10689}, 2018.

\end{thebibliography}
