\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{abbreviations}{glg-abr}{gls-abr}{glo-abr}
\providecommand\@glsxtr@savepreloctag[2]{}
\@newglossary{symbols}{symbols-glg}{symbols-gls}{symbols-glo}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{thesisDefense.ist}
\@glsorder{word}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{viii}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Figures}{ix}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{gordy2010nested}
\citation{gordy2010nested}
\citation{broadie2015risk}
\citation{broadie2015risk}
\citation{hong2017kernel}
\citation{feng2020optimal}
\citation{zhang2022sample}
\citation{giles2019multilevel}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Nested Simulation Procedures in Financial Engineering: A Selected Review}{2}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{2}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Problem Formulation}{3}{section.2.2}\protected@file@percent }
\newlabel{sec:problem-formulation}{{2.2}{3}{Problem Formulation}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Standard Nested Simulation Procedure}{4}{subsection.2.2.1}\protected@file@percent }
\citation{gordy2010nested}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Supervised Learning Models}{5}{subsection.2.2.2}\protected@file@percent }
\citation{broadie2015risk}
\citation{hong2017kernel}
\citation{wang2022smooth}
\citation{zhang2022sample}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Likelihood Ratio Method}{7}{subsection.2.2.3}\protected@file@percent }
\citation{giles2015multilevel}
\citation{giles2019multilevel}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Multi-level Monte Carlo}{8}{subsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Problem Statement}{8}{subsection.2.2.5}\protected@file@percent }
\citation{hastie2009elements,lecun2015deep}
\citation{silver2016mastering}
\citation{chatgpt}
\citation{mcculloch1943logical}
\citation{rosenblatt1958perceptron}
\citation{rumelhart1985learning}
\citation{williams1989learning,sutskever2014sequence}
\citation{hochreiter1997long,chung2014empirical}
\citation{poole2014analyzing}
\citation{neelakantan2015adding}
\citation{luo2016understanding}
\citation{srivastava2014dropout}
\citation{szegedy2013intriguing}
\citation{goodfellow2014explaining}
\citation{carlini2017towards}
\citation{jiang2020beyond}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Cutting Through the Noise: Using Deep Neural Network Metamodels for High Dimensional Nested Simulation}{9}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{9}{section.3.1}\protected@file@percent }
\citation{fonseca2003simulation}
\citation{lieu2022adaptive}
\citation{salle2014efficient}
\citation{liu2010stochastic}
\citation{gan2015valuation}
\citation{broadie2015risk}
\citation{hong2017kernel}
\citation{zhang2022sample}
\citation{rockafellar2002conditional}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Problem Formulation}{12}{section.3.2}\protected@file@percent }
\newlabel{sec2:problem-formulation}{{3.2}{12}{Problem Formulation}{section.3.2}{}}
\citation{eiopa2014underlying}
\citation{osfi2017life}
\citation{geneva2013variable}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Tail Risk Measures: VaR and CVaR}{13}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Simulation Model for Variable Annuity Payouts}{13}{subsection.3.2.2}\protected@file@percent }
\newlabel{subsec:VApayout}{{3.2.2}{13}{Simulation Model for Variable Annuity Payouts}{subsection.3.2.2}{}}
\citation{hardy2003investment}
\citation{dang2021efficient}
\citation{cathcart2015calculating}
\citation{glasserman2004monte}
\newlabel{eq2:delta}{{3.1}{15}{Simulation Model for Variable Annuity Payouts}{equation.3.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Dynamic Hedging for Variable Annuities}{16}{subsection.3.2.3}\protected@file@percent }
\newlabel{subsec:dynamicHedge}{{3.2.3}{16}{Dynamic Hedging for Variable Annuities}{subsection.3.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Illustration of multi-period nested simulation that estimates the P\&L for one outer scenario.}}{16}{figure.caption.8}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig2:illustration}{{3.1}{16}{Illustration of multi-period nested simulation that estimates the P\&L for one outer scenario}{figure.caption.8}{}}
\newlabel{eq2:hedgingerror}{{3.2}{17}{Dynamic Hedging for Variable Annuities}{equation.3.2.2}{}}
\newlabel{eq2:lossrv}{{3.3}{17}{Dynamic Hedging for Variable Annuities}{equation.3.2.3}{}}
\citation{dang2020efficient}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Standard Nested Simulation Procedure for Estimating CVaR for GMWB Hedging Losses}}{18}{algorithm.1}\protected@file@percent }
\newlabel{alg2:standardProcedure}{{1}{18}{Standard Nested Simulation Procedure for Estimating CVaR for GMWB Hedging Losses}{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Two-Stage Nested Simulation with Metamodels}{18}{section.3.3}\protected@file@percent }
\newlabel{sec2:metamodel2Stage}{{3.3}{18}{Two-Stage Nested Simulation with Metamodels}{section.3.3}{}}
\citation{dang2020efficient}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Two-Stage Metamodeling Nested Simulation Procedure for Estimating CVaR}}{19}{algorithm.2}\protected@file@percent }
\newlabel{alg2:twoStageProcedure}{{2}{19}{Two-Stage Metamodeling Nested Simulation Procedure for Estimating CVaR}{algorithm.2}{}}
\citation{dang2020efficient}
\citation{dang2020efficient}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Single-Stage Nested Simulation with Neural Network Metamodels}{21}{section.3.4}\protected@file@percent }
\newlabel{sec2:metamodel1Stage}{{3.4}{21}{Single-Stage Nested Simulation with Neural Network Metamodels}{section.3.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Single-Stage Metamodeling Nested Simulation Procedure for Estimating CVaR}}{21}{algorithm.3}\protected@file@percent }
\newlabel{alg2:oneStageProcedure}{{3}{21}{Single-Stage Metamodeling Nested Simulation Procedure for Estimating CVaR}{algorithm.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Numerical Results}{21}{section.3.5}\protected@file@percent }
\newlabel{sec2:numerical}{{3.5}{21}{Numerical Results}{section.3.5}{}}
\citation{kingma2014adam}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Architectures and MSEs of metamodels for GMWB inner simulation model.}}{23}{table.caption.9}\protected@file@percent }
\newlabel{tab:gmwb_arch}{{3.1}{23}{Architectures and MSEs of metamodels for GMWB inner simulation model}{table.caption.9}{}}
\newlabel{subfig2:badRNN}{{3.2b}{25}{A bad RNN metamodel}{figure.caption.10}{}}
\newlabel{sub@subfig2:badRNN}{{b}{25}{A bad RNN metamodel}{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces QQ-plots between true labels (x-axis) and predicted losses (y-axis) for the RNN metamodel.}}{25}{figure.caption.10}\protected@file@percent }
\newlabel{fig2:QQ_RNN}{{3.2}{25}{QQ-plots between true labels (x-axis) and predicted losses (y-axis) for the RNN metamodel}{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces QQ-plots between true losses (x-axis) and predicted losses (y-axis) for regression metamodels.}}{25}{figure.caption.11}\protected@file@percent }
\newlabel{fig2:QQ_REG}{{3.3}{25}{QQ-plots between true losses (x-axis) and predicted losses (y-axis) for regression metamodels}{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces QQ-plots between true losses (x-axis) and predicted losses (y-axis) for neural network metamodels.}}{26}{figure.caption.12}\protected@file@percent }
\newlabel{fig2:QQ_NN}{{3.4}{26}{QQ-plots between true losses (x-axis) and predicted losses (y-axis) for neural network metamodels}{figure.caption.12}{}}
\citation{dang2020efficient}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Two-Stage Procedure}{27}{subsection.3.5.1}\protected@file@percent }
\newlabel{subsec:twoStageProcedure}{{3.5.1}{27}{Two-Stage Procedure}{subsection.3.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Percentage of correctly identified true tail scenarios by different metamodels.}}{27}{figure.caption.13}\protected@file@percent }
\newlabel{fig2:tailMatches}{{3.5}{27}{Percentage of correctly identified true tail scenarios by different metamodels}{figure.caption.13}{}}
\newlabel{subfig2:AllSafetyMargin}{{3.6a}{28}{Safety margin 0\% - 15\%}{figure.caption.14}{}}
\newlabel{sub@subfig2:AllSafetyMargin}{{a}{28}{Safety margin 0\% - 15\%}{figure.caption.14}{}}
\newlabel{subfig2:ZoomedSafetyMargin}{{3.6b}{28}{Safety margin 7.5\% - 15\%}{figure.caption.14}{}}
\newlabel{sub@subfig2:ZoomedSafetyMargin}{{b}{28}{Safety margin 7.5\% - 15\%}{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Average 95\%-CVaR estimates by different procedures. Right figure is a zoomed-in version of left figure.}}{28}{figure.caption.14}\protected@file@percent }
\newlabel{fig2:CVaR95}{{3.6}{28}{Average 95\%-CVaR estimates by different procedures. Right figure is a zoomed-in version of left figure}{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Noise Tolerance of Deep Neural Network Metamodels}{29}{subsection.3.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces MSEs of LSTM metamodels.}}{30}{table.caption.15}\protected@file@percent }
\newlabel{tab:lstm_arch}{{3.2}{30}{MSEs of LSTM metamodels}{table.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces QQ-plots between true losses (x-axis) and predicted losses (y-axis) for two LSTM metamodels.}}{31}{figure.caption.16}\protected@file@percent }
\newlabel{fig2:QQ_All}{{3.7}{31}{QQ-plots between true losses (x-axis) and predicted losses (y-axis) for two LSTM metamodels}{figure.caption.16}{}}
\citation{broadie2015risk}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces MSE between regular LSTM predicted losses and true losses.}}{32}{table.caption.17}\protected@file@percent }
\newlabel{tab:lstm_sens}{{3.3}{32}{MSE between regular LSTM predicted losses and true losses}{table.caption.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces MSE between high-capacity LSTM predicted losses and true losses.}}{33}{table.caption.18}\protected@file@percent }
\newlabel{tab:hicaplstm_sens}{{3.4}{33}{MSE between high-capacity LSTM predicted losses and true losses}{table.caption.18}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Spearman (and Pearson) correlation coefficients between regular LSTM predicted losses and true losses.}}{33}{table.caption.19}\protected@file@percent }
\newlabel{tab:lstm_corr}{{3.5}{33}{Spearman (and Pearson) correlation coefficients between regular LSTM predicted losses and true losses}{table.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Single-Stage Procedure}{34}{subsection.3.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces CVaR estimates of the single-stage procedure with metamodels.}}{35}{figure.caption.20}\protected@file@percent }
\newlabel{fig2:CVaRsingleStage}{{3.8}{35}{CVaR estimates of the single-stage procedure with metamodels}{figure.caption.20}{}}
\newlabel{subfig2:convLoCap}{{3.9a}{36}{regular LSTM}{figure.caption.21}{}}
\newlabel{sub@subfig2:convLoCap}{{a}{36}{regular LSTM}{figure.caption.21}{}}
\newlabel{subfig2:convHiCap}{{3.9b}{36}{high-capacity LSTM}{figure.caption.21}{}}
\newlabel{sub@subfig2:convHiCap}{{b}{36}{high-capacity LSTM}{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Empirical convergence of CVaR for the single-stage procedure with LSTM metamodels.}}{36}{figure.caption.21}\protected@file@percent }
\newlabel{fig2:gammaConvergence}{{3.9}{36}{Empirical convergence of CVaR for the single-stage procedure with LSTM metamodels}{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Empirical convergence of the single-stage procedure with a LSTM metamodel.}}{37}{figure.caption.22}\protected@file@percent }
\newlabel{fig2:mnConvergence}{{3.10}{37}{Empirical convergence of the single-stage procedure with a LSTM metamodel}{figure.caption.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Conclusion}{37}{section.3.6}\protected@file@percent }
\newlabel{sec2:conclusion}{{3.6}{37}{Conclusion}{section.3.6}{}}
\bibstyle{apalike}
\bibdata{refP1,refP2,refP3}
\bibcite{artzner1999coherent}{{1}{1999}{{Artzner et~al.}}{{}}}
\bibcite{bauer2008universal}{{2}{2008}{{Bauer et~al.}}{{}}}
\bibcite{bauer2012calculation}{{3}{2012}{{Bauer et~al.}}{{}}}
\bibcite{boyle2003guaranteed}{{4}{2003}{{Boyle and Hardy}}{{}}}
\bibcite{boyle1997reserving}{{5}{1997}{{Boyle and Hardy}}{{}}}
\bibcite{boyle1977equilibrium}{{6}{1977}{{Boyle and Schwartz}}{{}}}
\bibcite{broadie2015risk}{{7}{2015}{{Broadie et~al.}}{{}}}
\bibcite{carlini2017towards}{{8}{2017}{{Carlini and Wagner}}{{}}}
\bibcite{caruana2000overfitting}{{9}{2000}{{Caruana et~al.}}{{}}}
\bibcite{cathcart2015calculating}{{10}{2015}{{Cathcart et~al.}}{{}}}
\@writefile{toc}{\contentsline {chapter}{\textbf  {References}}{40}{section*.23}\protected@file@percent }
\bibcite{chong2023pseudo}{{11}{2023}{{Chong et~al.}}{{}}}
\bibcite{chung2014empirical}{{12}{2014}{{Chung et~al.}}{{}}}
\bibcite{dang2021efficient}{{13}{2021}{{Dang}}{{}}}
\bibcite{dang2020efficient}{{14}{2020}{{Dang et~al.}}{{}}}
\bibcite{dang2022dynamic}{{15}{2022}{{Dang et~al.}}{{}}}
\bibcite{dang2023two}{{16}{2023}{{Dang et~al.}}{{}}}
\bibcite{eiopa2014underlying}{{17}{2014}{{EIOPA}}{{}}}
\bibcite{feng2020optimal}{{18}{2020}{{Feng and Song}}{{}}}
\bibcite{feng2016nested}{{19}{2016}{{Feng et~al.}}{{}}}
\bibcite{feng2022variable}{{20}{2022}{{Feng et~al.}}{{}}}
\bibcite{feng2017analytical}{{21}{2017}{{Feng and Jing}}{{}}}
\bibcite{fonseca2003simulation}{{22}{2003}{{Fonseca et~al.}}{{}}}
\bibcite{gan2013application}{{23}{2013}{{Gan}}{{}}}
\bibcite{gan2015valuation}{{24}{2015}{{Gan and Lin}}{{}}}
\bibcite{giles2015multilevel}{{25}{2015}{{Giles}}{{}}}
\bibcite{giles2019multilevel}{{26}{2019}{{Giles and Haji-Ali}}{{}}}
\bibcite{glasserman2004monte}{{27}{2004}{{Glasserman}}{{}}}
\bibcite{goodfellow2014explaining}{{28}{2014}{{Goodfellow et~al.}}{{}}}
\bibcite{gordy2010nested}{{29}{2010}{{Gordy and Juneja}}{{}}}
\bibcite{bauer2015least}{{30}{2015}{{Ha and Bauer}}{{}}}
\bibcite{hardle1990applied}{{31}{1990}{{H{\"a}rdle}}{{}}}
\bibcite{hardy2001regime}{{32}{2001}{{Hardy}}{{}}}
\bibcite{hardy2003investment}{{33}{2003}{{Hardy}}{{}}}
\bibcite{hardy2006introduction}{{34}{2006}{{Hardy}}{{}}}
\bibcite{hastie2009elements}{{35}{2009}{{Hastie et~al.}}{{}}}
\bibcite{hochreiter1997long}{{36}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{hong2014monte}{{37}{2014}{{Hong et~al.}}{{}}}
\bibcite{hong2017kernel}{{38}{2017}{{Hong et~al.}}{{}}}
\bibcite{jabbar2015methods}{{39}{2015}{{Jabbar and Khan}}{{}}}
\bibcite{jennen1988unifying}{{40}{1988}{{Jennen-Steinmetz and Gasser}}{{}}}
\bibcite{jiang2020beyond}{{41}{2020}{{Jiang et~al.}}{{}}}
\bibcite{kingma2014adam}{{42}{2014}{{Kingma and Ba}}{{}}}
\bibcite{krizhevsky2017imagenet}{{43}{2017}{{Krizhevsky et~al.}}{{}}}
\bibcite{lecun2015deep}{{44}{2015}{{LeCun et~al.}}{{}}}
\bibcite{lecun1998gradient}{{45}{1998}{{LeCun et~al.}}{{}}}
\bibcite{lieu2022adaptive}{{46}{2022}{{Lieu et~al.}}{{}}}
\bibcite{lin2020efficient}{{47}{2020a}{{Lin and Yang}}{{}}}
\bibcite{lin2020fast}{{48}{2020b}{{Lin and Yang}}{{}}}
\bibcite{liu2010stochastic}{{49}{2010}{{Liu and Staum}}{{}}}
\bibcite{longstaff2001valuing}{{50}{2001}{{Longstaff and Schwartz}}{{}}}
\bibcite{luo2016understanding}{{51}{2016}{{Luo et~al.}}{{}}}
\bibcite{mack1981local}{{52}{1981}{{Mack}}{{}}}
\bibcite{marshall2010valuation}{{53}{2010}{{Marshall et~al.}}{{}}}
\bibcite{marukame2016error}{{54}{2016}{{Marukame et~al.}}{{}}}
\bibcite{mcculloch1943logical}{{55}{1943}{{McCulloch and Pitts}}{{}}}
\bibcite{naic2021}{{56}{2021}{{NAIC}}{{}}}
\bibcite{neelakantan2015adding}{{57}{2015}{{Neelakantan et~al.}}{{}}}
\bibcite{chatgpt}{{58}{2023}{{OpenAI}}{{}}}
\bibcite{osfi2017life}{{59}{2017}{{OSFI}}{{}}}
\bibcite{peng2020empirical}{{60}{2020}{{Peng and Nagata}}{{}}}
\bibcite{piscopo2011valuation}{{61}{2011}{{Piscopo and Haberman}}{{}}}
\bibcite{poole2014analyzing}{{62}{2014}{{Poole et~al.}}{{}}}
\bibcite{stable-baselines3}{{63}{2021}{{Raffin et~al.}}{{}}}
\bibcite{rockafellar2002conditional}{{64}{2002}{{Rockafellar and Uryasev}}{{}}}
\bibcite{rosenblatt1958perceptron}{{65}{1958}{{Rosenblatt}}{{}}}
\bibcite{ruf2022hedging}{{66}{2022}{{Ruf and Wang}}{{}}}
\bibcite{rumelhart1985learning}{{67}{1985}{{Rumelhart et~al.}}{{}}}
\bibcite{salle2014efficient}{{68}{2014}{{Salle and Y{\i }ld{\i }zo{\u {g}}lu}}{{}}}
\bibcite{silver2016mastering}{{69}{2016}{{Silver et~al.}}{{}}}
\bibcite{srivastava2014dropout}{{70}{2014}{{Srivastava et~al.}}{{}}}
\bibcite{sutskever2014sequence}{{71}{2014}{{Sutskever et~al.}}{{}}}
\bibcite{szegedy2013intriguing}{{72}{2013}{{Szegedy et~al.}}{{}}}
\bibcite{geneva2013variable}{{73}{2013}{{The Geneva Association}}{{}}}
\bibcite{torres2017fault}{{74}{2017}{{Torres-Huitzil and Girau}}{{}}}
\bibcite{wang2022smooth}{{75}{2022}{{Wang et~al.}}{{}}}
\bibcite{williams1989learning}{{76}{1989}{{Williams and Zipser}}{{}}}
\bibcite{wirch1999synthesis}{{77}{1999}{{Wirch and Hardy}}{{}}}
\bibcite{yang2018bit}{{78}{2018}{{Yang et~al.}}{{}}}
\bibcite{zhang2022sample}{{79}{2022}{{Zhang et~al.}}{{}}}
\bibcite{zhang2021bootstrap}{{80}{2021}{{Zhang et~al.}}{{}}}
\citation{*}
\gdef \@abspage@last{55}
