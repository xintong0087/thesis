\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{abbreviations}{glg-abr}{gls-abr}{glo-abr}
\providecommand\@glsxtr@savepreloctag[2]{}
\@newglossary{symbols}{symbols-glg}{symbols-gls}{symbols-glo}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{thesisDefense.ist}
\@glsorder{word}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{x}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Figures}{xi}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Abbreviations}{xiv}{section*.7}\protected@file@percent }
\citation{glasserman2004monte}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{hardy2003investment}
\citation{hull2016options}
\citation{gordy2010nested}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Risk Management of Variable Annuities with Nested Simulation}{3}{section.1.1}\protected@file@percent }
\citation{kleijnen2018design}
\citation{glasserman2004monte}
\citation{jin2020deep}
\citation{tang2020deep}
\citation{rosen2012metamodeling}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Metamodeling for Monte Carlo Simulation}{4}{section.1.2}\protected@file@percent }
\citation{galton1886regression}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Machine Learning for Risk Management Applications}{5}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Supervised Learning Models}{5}{subsection.1.3.1}\protected@file@percent }
\citation{bishop2006pattern}
\citation{seber2012linear}
\newlabel{eq:mse}{{1.1}{6}{Supervised Learning Models}{equation.1.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Parametric Regression Models}{6}{subsection.1.3.2}\protected@file@percent }
\citation{szeg1939orthogonal}
\citation{hastie2009elements}
\citation{hastie2009elements}
\newlabel{eq:regression}{{1.2}{7}{Parametric Regression Models}{equation.1.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Non-Parametric Regression Models}{7}{subsection.1.3.3}\protected@file@percent }
\citation{bellman1966dynamic}
\citation{goodfellow2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Feedforward Neural Networks}{8}{subsection.1.3.4}\protected@file@percent }
\citation{nair2010rectified}
\citation{lecun2015deep}
\citation{lecun2015deep}
\citation{bengio2013representation}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{subfig:fnn}{{1.1a}{9}{\gls {fnn}}{figure.caption.10}{}}
\newlabel{sub@subfig:fnn}{{a}{9}{\gls {fnn}}{figure.caption.10}{}}
\newlabel{subfig:lstm}{{1.1b}{9}{\gls {lstm}}{figure.caption.10}{}}
\newlabel{sub@subfig:lstm}{{b}{9}{\gls {lstm}}{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Neural Network Architectures}}{9}{figure.caption.10}\protected@file@percent }
\newlabel{fig:nn}{{1.1}{9}{Neural Network Architectures}{figure.caption.10}{}}
\newlabel{eq:neural}{{1.5}{9}{Feedforward Neural Networks}{equation.1.3.5}{}}
\citation{hornik1989multilayer}
\citation{lecun2015deep}
\citation{krizhevsky2012imagenet}
\citation{simonyan2014very}
\citation{he2016deep}
\citation{cont2001empirical}
\citation{elman1990finding}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.5}Long Short-Term Memory Networks}{10}{subsection.1.3.5}\protected@file@percent }
\newlabel{subsec:LSTM}{{1.3.5}{10}{Long Short-Term Memory Networks}{subsection.1.3.5}{}}
\citation{bengio1994learning}
\citation{hochreiter1997long}
\citation{bengio2016}
\citation{kingma2014adam}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.6}Training and Evaluation of Neural Networks in a Simulation Environment}{12}{subsection.1.3.6}\protected@file@percent }
\citation{srivastava2014dropout}
\citation{prechelt2002early}
\citation{bishop2006pattern}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Different Levels of Noise}}{14}{figure.caption.11}\protected@file@percent }
\newlabel{fig:datasets}{{1.2}{14}{Different Levels of Noise}{figure.caption.11}{}}
\citation{gordy2010nested}
\citation{gordy2010nested}
\citation{broadie2015risk}
\citation{broadie2015risk}
\citation{hong2017kernel}
\citation{feng2020optimal}
\citation{zhang2022sample}
\citation{barton1998simulation}
\citation{giles2019multilevel}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Nested Simulation Procedures in Financial Engineering: A Selected Review}{16}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:project1}{{2}{16}{Nested Simulation Procedures in Financial Engineering: A Selected Review}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{16}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Problem Formulation}{18}{section.2.2}\protected@file@percent }
\newlabel{sec1:problem-formulation}{{2.2}{18}{Problem Formulation}{section.2.2}{}}
\newlabel{def1:smooth}{{1}{19}{Problem Formulation}{definition.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Smoothness assumption of $h$ for different nested simulation procedures}}{19}{table.caption.12}\protected@file@percent }
\newlabel{tab1:smoothness}{{2.1}{19}{Smoothness assumption of $h$ for different nested simulation procedures}{table.caption.12}{}}
\newlabel{def1:lipschitz}{{2}{19}{Problem Formulation}{definition.2}{}}
\citation{hardy2022quantitative}
\newlabel{def1:indicator}{{3}{20}{Problem Formulation}{definition.3}{}}
\newlabel{eq1:var}{{2.1}{20}{Problem Formulation}{equation.2.2.1}{}}
\newlabel{eq1:cvar}{{2.2}{20}{Problem Formulation}{equation.2.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Standard Nested Simulation Procedure}{20}{subsection.2.2.1}\protected@file@percent }
\citation{gordy2010nested}
\citation{giles2015multilevel}
\citation{giles2019multilevel}
\newlabel{eq1:cvar-hat}{{2.3}{21}{The Standard Nested Simulation Procedure}{equation.2.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Multi-level Monte Carlo}{21}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Supervised Learning Metamodels}{22}{subsection.2.2.3}\protected@file@percent }
\newlabel{eq1:sl-E}{{2.4}{22}{Supervised Learning Metamodels}{equation.2.2.4}{}}
\citation{broadie2015risk}
\citation{hong2017kernel}
\citation{wang2022smooth}
\newlabel{eq1:sl-train}{{2.5}{23}{Supervised Learning Metamodels}{equation.2.2.5}{}}
\newlabel{eq1:sl-test}{{2.6}{23}{Supervised Learning Metamodels}{equation.2.2.6}{}}
\citation{broadie2015risk}
\citation{nadaraya1964estimating}
\citation{watson1964smooth}
\citation{hong2017kernel}
\citation{mack1981local}
\citation{hong2017kernel}
\citation{wang2022smooth}
\citation{wang2022smooth}
\citation{zhang2022sample}
\newlabel{eq1:ae}{{2.8}{26}{Supervised Learning Metamodels}{equation.2.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Likelihood Ratio Method}{26}{subsection.2.2.4}\protected@file@percent }
\citation{gordy2010nested}
\citation{giles2019multilevel}
\citation{broadie2015risk}
\citation{hong2017kernel}
\citation{zhang2022sample}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Problem Statement}{27}{subsection.2.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Asymptotic Analysis}{27}{section.2.3}\protected@file@percent }
\newlabel{sec1:asymptotic-convergence}{{2.3}{27}{Asymptotic Analysis}{section.2.3}{}}
\citation{wang2022smooth}
\citation{wang2022smooth}
\citation{gordy2010nested}
\citation{wang2022smooth}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Existing asymptotic convergence results of nested simulation procedures for MSE}}{28}{table.caption.13}\protected@file@percent }
\newlabel{tab1:asymConv-mse}{{2.2}{28}{Existing asymptotic convergence results of nested simulation procedures for MSE}{table.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Connections between Convergence in \gls {mse} and AE}{29}{subsection.2.3.1}\protected@file@percent }
\newlabel{sec1:connection-mse-absolute-error}{{2.3.1}{29}{Connections between Convergence in \gls {mse} and AE}{subsection.2.3.1}{}}
\newlabel{thm1:convergence-mse-prob}{{1}{30}{Connections between Convergence in \gls {mse} and AE}{theorem.2.3.1}{}}
\citation{wang2022smooth}
\citation{gordy2010nested}
\citation{gordy2010nested}
\citation{hong2017kernel}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Asymptotic Analysis for the Standard Nested Simulation Procedure}{31}{subsection.2.3.2}\protected@file@percent }
\newlabel{sec1:sns}{{2.3.2}{31}{Asymptotic Analysis for the Standard Nested Simulation Procedure}{subsection.2.3.2}{}}
\citation{gordy2010nested}
\newlabel{as1:sns}{{1}{32}{Asymptotic Analysis for the Standard Nested Simulation Procedure}{assumption.1}{}}
\newlabel{as1:sns-noise}{{2}{32}{Asymptotic Analysis for the Standard Nested Simulation Procedure}{assumption.2}{}}
\newlabel{eq1:mse-sns}{{2.10}{32}{Asymptotic Analysis for the Standard Nested Simulation Procedure}{equation.2.3.10}{}}
\citation{wang2022smooth}
\newlabel{eq1:mse-sns2}{{2.11}{33}{Asymptotic Analysis for the Standard Nested Simulation Procedure}{equation.2.3.11}{}}
\newlabel{as1:sns-smooth}{{3}{33}{A Smooth Function $h$}{assumption.3}{}}
\newlabel{eq1:taylor-sns}{{2.12}{34}{A Smooth Function $h$}{equation.2.3.12}{}}
\newlabel{eq1:s1-sns}{{2.13}{35}{A Smooth Function $h$}{equation.2.3.13}{}}
\newlabel{as1:sns-noise-var}{{4}{35}{A Smooth Function $h$}{assumption.4}{}}
\citation{gordy2010nested}
\newlabel{eq1:s2-sns}{{2.14}{36}{A Smooth Function $h$}{equation.2.3.14}{}}
\newlabel{eq1:mse-sns-smooth}{{2.15}{36}{A Smooth Function $h$}{equation.2.3.15}{}}
\newlabel{eq1:bias-sns-smooth}{{2.16}{37}{A Smooth Function $h$}{equation.2.3.16}{}}
\newlabel{eq1:bias-sns-smooth-Taylor1}{{2.17}{37}{A Smooth Function $h$}{equation.2.3.17}{}}
\newlabel{eq1:bias-sns-smooth-Taylor2}{{2.18}{37}{A Smooth Function $h$}{equation.2.3.18}{}}
\newlabel{eq1:var-sns-smooth}{{2.19}{37}{A Smooth Function $h$}{equation.2.3.19}{}}
\citation{gordy2010nested}
\citation{gordy2010nested}
\citation{gordy2010nested}
\citation{broadie2015risk}
\newlabel{thm1:sns-smooth}{{2}{38}{A Smooth Function $h$}{theorem.2.3.2}{}}
\newlabel{as1:sns-lip}{{5}{38}{A Lipschitz Continuous Function $h$}{assumption.5}{}}
\citation{gordy2010nested}
\newlabel{eq1:bias-sns-lip}{{2.22}{39}{A Lipschitz Continuous Function $h$}{equation.2.3.22}{}}
\citation{hong2017kernel}
\citation{hong2017kernel}
\citation{hong2017kernel}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Asymptotic Analysis of a kNN-based Nested Simulation Procedure for a Smooth Function}{40}{subsection.2.3.3}\protected@file@percent }
\citation{mack1981local}
\newlabel{as1:knn}{{6}{41}{Asymptotic Analysis of a kNN-based Nested Simulation Procedure for a Smooth Function}{assumption.6}{}}
\newlabel{eq1:bias-knn}{{2.24}{42}{Asymptotic Analysis of a kNN-based Nested Simulation Procedure for a Smooth Function}{equation.2.3.24}{}}
\newlabel{eq1:bias-knn-1}{{2.25}{42}{Asymptotic Analysis of a kNN-based Nested Simulation Procedure for a Smooth Function}{equation.2.3.25}{}}
\citation{hong2017kernel}
\newlabel{eq1:bias-knn-2}{{2.26}{43}{Asymptotic Analysis of a kNN-based Nested Simulation Procedure for a Smooth Function}{equation.2.3.26}{}}
\newlabel{def:knn-smooth-bias}{{6}{43}{Asymptotic Analysis of a kNN-based Nested Simulation Procedure for a Smooth Function}{definition.6}{}}
\citation{gordy2010nested}
\citation{broadie2015risk}
\citation{gordy2010nested}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Convergence Orders and Critical Assumptions of Nested Simulation Procedures}{44}{section.2.4}\protected@file@percent }
\newlabel{sec1:convergence-orders}{{2.4}{44}{Convergence Orders and Critical Assumptions of Nested Simulation Procedures}{section.2.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Asymptotic rate of convergence of nested simulation procedures in \gls {mse}}}{44}{table.caption.16}\protected@file@percent }
\newlabel{tab1:asymConv-order}{{2.3}{44}{Asymptotic rate of convergence of nested simulation procedures in \gls {mse}}{table.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Standard Assumptions}{44}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Assumptions on Joint Density}{44}{subsection.2.4.2}\protected@file@percent }
\newlabel{as1:sns-joint-density}{{7}{44}{Assumptions on Joint Density}{assumption.7}{}}
\citation{broadie2015risk}
\citation{giles2019multilevel}
\newlabel{as1:sns-joint-density-bound}{{8}{45}{Assumptions on Joint Density}{assumption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Assumptions for the MLMC Procedure}{45}{subsection.2.4.3}\protected@file@percent }
\newlabel{as1:mlmc-q}{{9}{45}{Assumptions for the MLMC Procedure}{assumption.9}{}}
\citation{hong2017kernel}
\citation{jennen1988unifying}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Assumptions for the Likelihood Ratio-Based Procedure}{46}{subsection.2.4.4}\protected@file@percent }
\newlabel{as1:likelihood-ratio-marginal-density}{{10}{46}{Assumptions for the Likelihood Ratio-Based Procedure}{assumption.10}{}}
\newlabel{as1:likelihood-ratio-independence}{{11}{46}{Assumptions for the Likelihood Ratio-Based Procedure}{assumption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}Assumptions for the Kernel-Based Procedure}{46}{subsection.2.4.5}\protected@file@percent }
\citation{zhang2021bootstrap}
\citation{frazier2018bayesian}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.6}Assumptions for the KRR-Based Procedure}{47}{subsection.2.4.6}\protected@file@percent }
\newlabel{as1:krr-domain}{{13}{47}{Assumptions for the KRR-Based Procedure}{assumption.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Finite-Sample Experiments}{47}{section.2.5}\protected@file@percent }
\newlabel{sec1:numerical-experiments}{{2.5}{47}{Finite-Sample Experiments}{section.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Empirical convergence of nested simulation procedures for quadratic tracking error on Portfolio 1 with $d=1$}}{49}{figure.caption.17}\protected@file@percent }
\newlabel{fig1:compareAll}{{2.1}{49}{Empirical convergence of nested simulation procedures for quadratic tracking error on Portfolio 1 with $d=1$}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Sensitivity to the Asset Dimension}{50}{subsection.2.5.1}\protected@file@percent }
\newlabel{sec1:sensitivity-dimension}{{2.5.1}{50}{Sensitivity to the Asset Dimension}{subsection.2.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Empirical convergence of nested simulation procedures for quadratic tracking error on Portfolio 1 with different asset dimensions}}{51}{figure.caption.18}\protected@file@percent }
\newlabel{fig1:assetDimension}{{2.2}{51}{Empirical convergence of nested simulation procedures for quadratic tracking error on Portfolio 1 with different asset dimensions}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Empirical Convergence of Kernel Smoothing-Based Procedures}{52}{subsection.2.5.2}\protected@file@percent }
\newlabel{sec1:kernel-smoothing-convergence}{{2.5.2}{52}{Empirical Convergence of Kernel Smoothing-Based Procedures}{subsection.2.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Empirical convergence of kernel smoothing procedure for different values of $d$}}{52}{figure.caption.19}\protected@file@percent }
\newlabel{fig1:kernel_d}{{2.3}{52}{Empirical convergence of kernel smoothing procedure for different values of $d$}{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Cross-validation for the kernel smoothing-based procedure with $\Gamma =\num {100000}$}}{53}{figure.caption.20}\protected@file@percent }
\newlabel{fig1:kernel_cv}{{2.4}{53}{Cross-validation for the kernel smoothing-based procedure with $\Gamma =\num {100000}$}{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Empirical Convergence of Parametric Regression Procedures}{54}{subsection.2.5.3}\protected@file@percent }
\newlabel{sec1:regression-convergence}{{2.5.3}{54}{Empirical Convergence of Parametric Regression Procedures}{subsection.2.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Empirical convergence of regression procedure for European call options and $d=20$}}{54}{figure.caption.21}\protected@file@percent }
\newlabel{fig1:reg_lb}{{2.5}{54}{Empirical convergence of regression procedure for European call options and $d=20$}{figure.caption.21}{}}
\citation{broadie2015risk}
\citation{broadie2015risk}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Empirical convergence of regression-based nested simulation procedures for different regression bases}}{56}{figure.caption.22}\protected@file@percent }
\newlabel{fig1:sens_basis}{{2.6}{56}{Empirical convergence of regression-based nested simulation procedures for different regression bases}{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}Sensitivity to the Option Types and Risk Measures}{56}{subsection.2.5.4}\protected@file@percent }
\newlabel{sec1:sensitivity-option-type}{{2.5.4}{56}{Sensitivity to the Option Types and Risk Measures}{subsection.2.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Empirical convergence of nested simulation procedures for quadratic tracking error on different portfolios with $d=20$}}{57}{figure.caption.23}\protected@file@percent }
\newlabel{fig1:1x03}{{2.7}{57}{Empirical convergence of nested simulation procedures for quadratic tracking error on different portfolios with $d=20$}{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Empirical convergence of nested simulation procedures for a W-shaped payoff}}{57}{figure.caption.24}\protected@file@percent }
\newlabel{fig1:5503}{{2.8}{57}{Empirical convergence of nested simulation procedures for a W-shaped payoff}{figure.caption.24}{}}
\citation{heston1993closed}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Empirical convergence of nested simulation procedures for different risk measures on Portfolio 1 with $d=20$}}{58}{figure.caption.25}\protected@file@percent }
\newlabel{fig1:110x}{{2.9}{58}{Empirical convergence of nested simulation procedures for different risk measures on Portfolio 1 with $d=20$}{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.5}Sensitivity to level for VaR and CVaR}{58}{subsection.2.5.5}\protected@file@percent }
\newlabel{sec1:sensitivity-level}{{2.5.5}{58}{Sensitivity to level for VaR and CVaR}{subsection.2.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.6}Sensitivity to the Asset Model}{58}{subsection.2.5.6}\protected@file@percent }
\newlabel{sec1:sensitivity-assetModel}{{2.5.6}{58}{Sensitivity to the Asset Model}{subsection.2.5.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Empirical convergence of regression-based procedures for different levels of VaR and CVaR for Up and Out Barrier Call Options}}{59}{figure.caption.26}\protected@file@percent }
\newlabel{fig1:sens_level}{{2.10}{59}{Empirical convergence of regression-based procedures for different levels of VaR and CVaR for Up and Out Barrier Call Options}{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Empirical convergence of regression-based nested simulation procedures for different asset models}}{59}{figure.caption.27}\protected@file@percent }
\newlabel{fig1:sens_model}{{2.11}{59}{Empirical convergence of regression-based nested simulation procedures for different asset models}{figure.caption.27}{}}
\citation{giles2019multilevel}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.7}Empirical Convergence of MLMC}{60}{subsection.2.5.7}\protected@file@percent }
\newlabel{sec1:empirical-mlmc}{{2.5.7}{60}{Empirical Convergence of MLMC}{subsection.2.5.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces MSEs of the MLMC procedure for different levels}}{60}{table.caption.28}\protected@file@percent }
\newlabel{tab1:mlmc-mse}{{2.4}{60}{MSEs of the MLMC procedure for different levels}{table.caption.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Computational Complexity}{61}{section.2.6}\protected@file@percent }
\newlabel{sec1:computational-complexity}{{2.6}{61}{Computational Complexity}{section.2.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.5}{\ignorespaces Additional computational costs of nested simulation procedures aside from simulation}}{61}{table.caption.29}\protected@file@percent }
\newlabel{tab1:complexity}{{2.5}{61}{Additional computational costs of nested simulation procedures aside from simulation}{table.caption.29}{}}
\citation{stothers2010complexity}
\citation{strassen1969gaussian}
\citation{coppersmith1987matrix}
\citation{bentley1975multidimensional}
\citation{kim2008ratio}
\citation{scholkopf2002learning}
\citation{shahriari2015taking}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Total computational cost for different procedures with $d=10$}}{64}{figure.caption.30}\protected@file@percent }
\newlabel{fig1:tcc}{{2.12}{64}{Total computational cost for different procedures with $d=10$}{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Computational cost for implementing nested simulation procedures with $d=10$, excluding simulation time}}{64}{figure.caption.31}\protected@file@percent }
\newlabel{fig1:c_model}{{2.13}{64}{Computational cost for implementing nested simulation procedures with $d=10$, excluding simulation time}{figure.caption.31}{}}
\citation{zhang2021bootstrap}
\citation{nystrom1930praktische}
\citation{scikit-learn}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Conclusion and Extensions}{66}{section.2.7}\protected@file@percent }
\newlabel{sec1:conclusion}{{2.7}{66}{Conclusion and Extensions}{section.2.7}{}}
\citation{hastie2009elements,lecun2015deep}
\citation{silver2016mastering}
\citation{chatgpt}
\citation{mcculloch1943logical}
\citation{rosenblatt1958perceptron}
\citation{rumelhart1985learning}
\citation{williams1989learning,sutskever2014sequence}
\citation{hochreiter1997long,chung2014empirical}
\citation{poole2014analyzing}
\citation{neelakantan2015adding}
\citation{luo2016understanding}
\citation{srivastava2014dropout}
\citation{szegedy2013intriguing}
\citation{goodfellow2014explaining}
\citation{carlini2017towards}
\citation{jiang2020beyond}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Cutting Through the Noise: Using Deep Neural Network Metamodels for High Dimensional Nested Simulation}{68}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:project2}{{3}{68}{Cutting Through the Noise: Using Deep Neural Network Metamodels for High Dimensional Nested Simulation}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{68}{section.3.1}\protected@file@percent }
\citation{fonseca2003simulation}
\citation{lieu2022adaptive}
\citation{salle2014efficient}
\citation{liu2010stochastic}
\citation{gan2015valuation}
\citation{broadie2015risk}
\citation{hong2017kernel}
\citation{zhang2022sample}
\citation{hardy2022quantitative,rockafellar2002conditional}
\citation{eiopa2014underlying}
\citation{osfi2017life}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Problem Formulation}{72}{section.3.2}\protected@file@percent }
\newlabel{sec2:problem-formulation}{{3.2}{72}{Problem Formulation}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Tail Risk Measures}{72}{subsection.3.2.1}\protected@file@percent }
\citation{geneva2013variable}
\citation{hardy2003investment}
\citation{dang2021efficient}
\newlabel{eq2:cvar-hat}{{3.1}{73}{Tail Risk Measures}{equation.3.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Simulation Model for Variable Annuity Payouts}{73}{subsection.3.2.2}\protected@file@percent }
\newlabel{subsec:VApayout}{{3.2.2}{73}{Simulation Model for Variable Annuity Payouts}{subsection.3.2.2}{}}
\citation{cathcart2015calculating}
\citation{glasserman2004monte}
\newlabel{eq2:delta}{{3.2}{75}{Simulation Model for Variable Annuity Payouts}{equation.3.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Dynamic Hedging for Variable Annuities}{75}{subsection.3.2.3}\protected@file@percent }
\newlabel{subsec:dynamicHedge}{{3.2.3}{75}{Dynamic Hedging for Variable Annuities}{subsection.3.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Illustration of a multi-period nested simulation that estimates the P\&L for one outer scenario.}}{76}{figure.caption.32}\protected@file@percent }
\newlabel{fig2:illustration}{{3.1}{76}{Illustration of a multi-period nested simulation that estimates the P\&L for one outer scenario}{figure.caption.32}{}}
\newlabel{eq2:hedgingerror}{{3.3}{76}{Dynamic Hedging for Variable Annuities}{equation.3.2.3}{}}
\newlabel{eq2:lossrv}{{3.4}{77}{Dynamic Hedging for Variable Annuities}{equation.3.2.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Standard Nested Simulation Procedure for Estimating CVaR for GMWB Hedging Losses}}{77}{algorithm.1}\protected@file@percent }
\newlabel{alg2:standardProcedure}{{1}{77}{Standard Nested Simulation Procedure for Estimating CVaR for GMWB Hedging Losses}{algorithm.1}{}}
\citation{dang2020efficient}
\citation{dang2020efficient}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Two-Stage Nested Simulation with Metamodels}{78}{section.3.3}\protected@file@percent }
\newlabel{sec2:metamodel2Stage}{{3.3}{78}{Two-Stage Nested Simulation with Metamodels}{section.3.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Two-Stage Metamodeling Nested Simulation Procedure for Estimating CVaR}}{79}{algorithm.2}\protected@file@percent }
\newlabel{alg2:twoStageProcedure}{{2}{79}{Two-Stage Metamodeling Nested Simulation Procedure for Estimating CVaR}{algorithm.2}{}}
\newlabel{eq2:return}{{3.5}{79}{Two-Stage Metamodeling Nested Simulation Procedure for Estimating CVaR}{equation.3.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Single-Stage Nested Simulation with Neural Network Metamodels}{81}{section.3.4}\protected@file@percent }
\newlabel{sec2:metamodel1Stage}{{3.4}{81}{Single-Stage Nested Simulation with Neural Network Metamodels}{section.3.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Single-Stage Metamodeling Nested Simulation Procedure for Estimating CVaR}}{81}{algorithm.3}\protected@file@percent }
\newlabel{alg2:oneStageProcedure}{{3}{81}{Single-Stage Metamodeling Nested Simulation Procedure for Estimating CVaR}{algorithm.3}{}}
\citation{dang2020efficient}
\citation{naic2021}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Numerical Results}{82}{section.3.5}\protected@file@percent }
\newlabel{sec2:numerical}{{3.5}{82}{Numerical Results}{section.3.5}{}}
\newlabel{eq:dynamic_lapse_rate}{{3.6}{82}{Numerical Results}{equation.3.5.6}{}}
\newlabel{eq:lapse_rate}{{3.7}{82}{Numerical Results}{equation.3.5.7}{}}
\citation{kingma2014adam}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Real-world parameters for the regime-switching model (monthly rates)}}{83}{table.caption.33}\protected@file@percent }
\newlabel{tab:regime_params}{{3.1}{83}{Real-world parameters for the regime-switching model (monthly rates)}{table.caption.33}{}}
\citation{krizhevsky2012imagenet}
\citation{devlin2018bert}
\citation{brown2020language}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Architectures and \gls {mse}s of metamodels for \gls {gmwb} inner simulation model.}}{84}{table.caption.34}\protected@file@percent }
\newlabel{tab:gmwb_arch}{{3.2}{84}{Architectures and \gls {mse}s of metamodels for \gls {gmwb} inner simulation model}{table.caption.34}{}}
\newlabel{subfig2:badRNN}{{3.2b}{88}{A bad RNN metamodel}{figure.caption.35}{}}
\newlabel{sub@subfig2:badRNN}{{b}{88}{A bad RNN metamodel}{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces QQ-plots between true labels (x-axis) and predicted losses (y-axis) for the RNN metamodel.}}{88}{figure.caption.35}\protected@file@percent }
\newlabel{fig2:QQ_RNN}{{3.2}{88}{QQ-plots between true labels (x-axis) and predicted losses (y-axis) for the RNN metamodel}{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces QQ-plots between true losses (x-axis) and predicted losses (y-axis) for regression metamodels.}}{89}{figure.caption.36}\protected@file@percent }
\newlabel{fig2:QQ_REG}{{3.3}{89}{QQ-plots between true losses (x-axis) and predicted losses (y-axis) for regression metamodels}{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces QQ-plots between true losses (x-axis) and predicted losses (y-axis) for neural network metamodels.}}{89}{figure.caption.37}\protected@file@percent }
\newlabel{fig2:QQ_NN}{{3.4}{89}{QQ-plots between true losses (x-axis) and predicted losses (y-axis) for neural network metamodels}{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Two-Stage Procedure}{90}{subsection.3.5.1}\protected@file@percent }
\newlabel{subsec:twoStageProcedure}{{3.5.1}{90}{Two-Stage Procedure}{subsection.3.5.1}{}}
\citation{dang2020efficient}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Percentage of correctly identified true tail scenarios by different metamodels.}}{91}{figure.caption.38}\protected@file@percent }
\newlabel{fig2:tailMatches}{{3.5}{91}{Percentage of correctly identified true tail scenarios by different metamodels}{figure.caption.38}{}}
\newlabel{subfig2:AllSafetyMargin}{{3.6a}{92}{Safety margin $0\%$ - $15\%$}{figure.caption.39}{}}
\newlabel{sub@subfig2:AllSafetyMargin}{{a}{92}{Safety margin $0\%$ - $15\%$}{figure.caption.39}{}}
\newlabel{subfig2:ZoomedSafetyMargin}{{3.6b}{92}{Safety margin $7.5\%$ - $15\%$}{figure.caption.39}{}}
\newlabel{sub@subfig2:ZoomedSafetyMargin}{{b}{92}{Safety margin $7.5\%$ - $15\%$}{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Average $95\%$-CVaR estimates by different procedures. The right figure is a zoomed-in version of the left figure.}}{92}{figure.caption.39}\protected@file@percent }
\newlabel{fig2:CVaR95}{{3.6}{92}{Average $95\%$-CVaR estimates by different procedures. The right figure is a zoomed-in version of the left figure}{figure.caption.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Noise Tolerance of DNN Metamodels}{93}{subsection.3.5.2}\protected@file@percent }
\newlabel{subsec2:noiseTolerance}{{3.5.2}{93}{Noise Tolerance of DNN Metamodels}{subsection.3.5.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces MSEs of LSTM metamodels.}}{94}{table.caption.40}\protected@file@percent }
\newlabel{tab:lstm_arch}{{3.3}{94}{MSEs of LSTM metamodels}{table.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces QQ-plots between true losses (x-axis) and predicted losses (y-axis) for two LSTM metamodels.}}{96}{figure.caption.41}\protected@file@percent }
\newlabel{fig2:QQ_All}{{3.7}{96}{QQ-plots between true losses (x-axis) and predicted losses (y-axis) for two LSTM metamodels}{figure.caption.41}{}}
\citation{broadie2015risk}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces MSE between regular LSTM predicted losses and true losses.}}{97}{table.caption.42}\protected@file@percent }
\newlabel{tab2:lstm_sens}{{3.4}{97}{MSE between regular LSTM predicted losses and true losses}{table.caption.42}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces MSE between high-capacity LSTM predicted losses and true losses.}}{97}{table.caption.43}\protected@file@percent }
\newlabel{tab2:hicaplstm_sens}{{3.5}{97}{MSE between high-capacity LSTM predicted losses and true losses}{table.caption.43}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces Spearman (Pearson) correlation coefficients of high-capacity LSTM predictions.}}{98}{table.caption.44}\protected@file@percent }
\newlabel{tab2:lstm_corr}{{3.6}{98}{Spearman (Pearson) correlation coefficients of high-capacity LSTM predictions}{table.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Difference between Spearman and Pearson correlations for high-capacity LSTM metamodel.}}{99}{figure.caption.45}\protected@file@percent }
\newlabel{fig2:cor-heatmap}{{3.8}{99}{Difference between Spearman and Pearson correlations for high-capacity LSTM metamodel}{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Single-Stage Procedure}{99}{subsection.3.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces CVaR estimates of the single-stage procedure with metamodels.}}{100}{figure.caption.46}\protected@file@percent }
\newlabel{fig2:CVaRsingleStage}{{3.9}{100}{CVaR estimates of the single-stage procedure with metamodels}{figure.caption.46}{}}
\newlabel{subfig2:convLoCap}{{3.10a}{101}{Regular LSTM}{figure.caption.47}{}}
\newlabel{sub@subfig2:convLoCap}{{a}{101}{Regular LSTM}{figure.caption.47}{}}
\newlabel{subfig2:convHiCap}{{3.10b}{101}{High-capacity LSTM}{figure.caption.47}{}}
\newlabel{sub@subfig2:convHiCap}{{b}{101}{High-capacity LSTM}{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Empirical convergence of CVaR for the single-stage procedure with LSTM metamodels.}}{101}{figure.caption.47}\protected@file@percent }
\newlabel{fig2:gammaConvergence}{{3.10}{101}{Empirical convergence of CVaR for the single-stage procedure with LSTM metamodels}{figure.caption.47}{}}
\citation{gordy2010nested}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Empirical convergence of the single-stage procedure with a LSTM metamodel.}}{102}{figure.caption.48}\protected@file@percent }
\newlabel{fig2:mnConvergence}{{3.11}{102}{Empirical convergence of the single-stage procedure with a LSTM metamodel}{figure.caption.48}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Conclusion}{103}{section.3.6}\protected@file@percent }
\newlabel{sec2:conclusion}{{3.6}{103}{Conclusion}{section.3.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Transfer Learning for Rapid Adaptation of Deep Neural Network Metamodels in Dynamic Hedging of Variable Annuities}{105}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:project3}{{4}{105}{Transfer Learning for Rapid Adaptation of Deep Neural Network Metamodels in Dynamic Hedging of Variable Annuities}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{105}{section.4.1}\protected@file@percent }
\citation{golestaneh2024many}
\citation{cont2001empirical}
\citation{pan2009survey}
\citation{yosinski2014transferable}
\citation{box1970distribution}
\citation{bollerslev1990modelling}
\citation{cont2001empirical}
\citation{hamilton1989new}
\citation{embrechts2013modelling}
\citation{jeong2019improving}
\citation{lebichot2021transfer}
\citation{yan2024comprehensive}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Transfer Learning in Financial Metamodeling}{107}{section.4.2}\protected@file@percent }
\newlabel{sec3:background}{{4.2}{107}{Transfer Learning in Financial Metamodeling}{section.4.2}{}}
\citation{cheng2019fast}
\citation{gan2015valuation}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Fine-tuning}{110}{subsection.4.2.1}\protected@file@percent }
\citation{caruana1997multitask}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Fine-tuning Metamodel for a Target Task}}{111}{algorithm.4}\protected@file@percent }
\newlabel{alg3:fineTuning}{{4}{111}{Fine-tuning Metamodel for a Target Task}{algorithm.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Layer Freezing}{111}{subsection.4.2.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Layer Freezing for Metamodel Transfer}}{112}{algorithm.5}\protected@file@percent }
\newlabel{alg3:layerFreezing}{{5}{112}{Layer Freezing for Metamodel Transfer}{algorithm.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Multi-task Learning}{112}{subsection.4.2.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Multi-task Learning Framework for \gls {lstm} Metamodels}}{113}{algorithm.6}\protected@file@percent }
\newlabel{alg3:multiTaskLearning}{{6}{113}{Multi-task Learning Framework for \gls {lstm} Metamodels}{algorithm.6}{}}
\newlabel{eq3:multiTaskLoss}{{4.6}{113}{Multi-task Learning Framework for \gls {lstm} Metamodels}{equation.4.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Rapid Adaptation of \gls {lstm} Metamodels}{115}{subsection.4.2.4}\protected@file@percent }
\newlabel{sec3:transfer_learning}{{4.2.4}{115}{Rapid Adaptation of \gls {lstm} Metamodels}{subsection.4.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Numerical Experiments}{115}{section.4.3}\protected@file@percent }
\newlabel{sec3:experiments}{{4.3}{115}{Numerical Experiments}{section.4.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Transfer Learning Framework for \gls {lstm} Metamodels: Combining Fine-tuning, Layer Freezing, and Multi-task Learning}}{116}{algorithm.7}\protected@file@percent }
\newlabel{alg3:combined}{{7}{116}{Transfer Learning Framework for \gls {lstm} Metamodels: Combining Fine-tuning, Layer Freezing, and Multi-task Learning}{algorithm.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces \gls {va} Contracts for Transfer Learning Experiments}}{117}{table.caption.49}\protected@file@percent }
\newlabel{tab3:contracts}{{4.1}{117}{\gls {va} Contracts for Transfer Learning Experiments}{table.caption.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Learning Lapse Features}{118}{subsection.4.3.1}\protected@file@percent }
\newlabel{subfig3-1:extensive}{{4.1a}{119}{Extensive Training on Target Task}{figure.caption.50}{}}
\newlabel{sub@subfig3-1:extensive}{{a}{119}{Extensive Training on Target Task}{figure.caption.50}{}}
\newlabel{subfig3-1:without}{{4.1b}{119}{Without \gls {tl}}{figure.caption.50}{}}
\newlabel{sub@subfig3-1:without}{{b}{119}{Without \gls {tl}}{figure.caption.50}{}}
\newlabel{subfig3-1:fineTuning}{{4.1c}{119}{With Fine-tuning}{figure.caption.50}{}}
\newlabel{sub@subfig3-1:fineTuning}{{c}{119}{With Fine-tuning}{figure.caption.50}{}}
\newlabel{subfig3-1:layerFreezing}{{4.1d}{119}{With Layer Freezing}{figure.caption.50}{}}
\newlabel{sub@subfig3-1:layerFreezing}{{d}{119}{With Layer Freezing}{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Metamodel performance on \gls {rsgbm} \gls {gmmb} with static lapse}}{119}{figure.caption.50}\protected@file@percent }
\newlabel{fig3:figure1}{{4.1}{119}{Metamodel performance on \gls {rsgbm} \gls {gmmb} with static lapse}{figure.caption.50}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Transfer to VAs with a Dynamic Lapse}{120}{subsection.4.3.2}\protected@file@percent }
\newlabel{subfig3-2:fromNolapse}{{4.2a}{120}{Fine-tuning from No Lapse}{figure.caption.51}{}}
\newlabel{sub@subfig3-2:fromNolapse}{{a}{120}{Fine-tuning from No Lapse}{figure.caption.51}{}}
\newlabel{subfig3-2:fromLapse}{{4.2b}{120}{Fine-tuning from Static Lapse}{figure.caption.51}{}}
\newlabel{sub@subfig3-2:fromLapse}{{b}{120}{Fine-tuning from Static Lapse}{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Fine-tuned Metamodel performance on \gls {rsgbm} \gls {gmmb} with a dynamic lapse}}{120}{figure.caption.51}\protected@file@percent }
\newlabel{fig3:figure2}{{4.2}{120}{Fine-tuned Metamodel performance on \gls {rsgbm} \gls {gmmb} with a dynamic lapse}{figure.caption.51}{}}
\newlabel{subfig3-3:freezeLSTM}{{4.3a}{121}{Freezing LSTM Layers}{figure.caption.52}{}}
\newlabel{sub@subfig3-3:freezeLSTM}{{a}{121}{Freezing LSTM Layers}{figure.caption.52}{}}
\newlabel{subfig3-3:freezeFC}{{4.3b}{121}{Freezing the Fully Connected Layer}{figure.caption.52}{}}
\newlabel{sub@subfig3-3:freezeFC}{{b}{121}{Freezing the Fully Connected Layer}{figure.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Layer Freezing on \gls {rsgbm} \gls {gmmb} with dynamic lapse}}{121}{figure.caption.52}\protected@file@percent }
\newlabel{fig3:figure3}{{4.3}{121}{Layer Freezing on \gls {rsgbm} \gls {gmmb} with dynamic lapse}{figure.caption.52}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Comparison of different \gls {tl} methods on \gls {gmmb} contracts}}{122}{table.caption.53}\protected@file@percent }
\newlabel{tab3:transfer_learning_results}{{4.2}{122}{Comparison of different \gls {tl} methods on \gls {gmmb} contracts}{table.caption.53}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Transfer Knowledge to other Contract Types}{123}{subsection.4.3.3}\protected@file@percent }
\newlabel{subfig3-4:extensive}{{4.4a}{123}{Extensive Training on Target Task}{figure.caption.54}{}}
\newlabel{sub@subfig3-4:extensive}{{a}{123}{Extensive Training on Target Task}{figure.caption.54}{}}
\newlabel{subfig3-4:without}{{4.4b}{123}{Without \gls {tl}}{figure.caption.54}{}}
\newlabel{sub@subfig3-4:without}{{b}{123}{Without \gls {tl}}{figure.caption.54}{}}
\newlabel{subfig3-4:fineTuning}{{4.4c}{123}{With Fine-tuning}{figure.caption.54}{}}
\newlabel{sub@subfig3-4:fineTuning}{{c}{123}{With Fine-tuning}{figure.caption.54}{}}
\newlabel{subfig3-4:layerFreezing}{{4.4d}{123}{With Layer Freezing}{figure.caption.54}{}}
\newlabel{sub@subfig3-4:layerFreezing}{{d}{123}{With Layer Freezing}{figure.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces TL performance on RS-GBM GMWB with dynamic lapse}}{123}{figure.caption.54}\protected@file@percent }
\newlabel{fig3:figure4}{{4.4}{123}{TL performance on RS-GBM GMWB with dynamic lapse}{figure.caption.54}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Comparison of different TL methods to GMWB contracts}}{124}{table.caption.55}\protected@file@percent }
\newlabel{tab3:transfer_learning_results_gmwb}{{4.3}{124}{Comparison of different TL methods to GMWB contracts}{table.caption.55}{}}
\newlabel{subfig3-4-1:tail}{{4.5a}{125}{Identifying Tail scenarios}{figure.caption.56}{}}
\newlabel{sub@subfig3-4-1:tail}{{a}{125}{Identifying Tail scenarios}{figure.caption.56}{}}
\newlabel{subfig3-4-2:CVaR}{{4.5b}{125}{$95\%$-CVaR predictions}{figure.caption.56}{}}
\newlabel{sub@subfig3-4-2:CVaR}{{b}{125}{$95\%$-CVaR predictions}{figure.caption.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces TL performance on \gls {rsgbm} GMWB with dynamic lapse}}{125}{figure.caption.56}\protected@file@percent }
\newlabel{fig3:figure4-1}{{4.5}{125}{TL performance on \gls {rsgbm} GMWB with dynamic lapse}{figure.caption.56}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Multi-task Learning}{126}{subsection.4.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Multi-task learning framework for VA contracts}}{126}{figure.caption.57}\protected@file@percent }
\newlabel{fig3:mtl}{{4.6}{126}{Multi-task learning framework for VA contracts}{figure.caption.57}{}}
\newlabel{subfig3-5:multiTask}{{4.7a}{127}{Multi-task training history}{figure.caption.58}{}}
\newlabel{sub@subfig3-5:multiTask}{{a}{127}{Multi-task training history}{figure.caption.58}{}}
\newlabel{subfig3-5:fineTuning}{{4.7b}{127}{Task performance with multi-task training}{figure.caption.58}{}}
\newlabel{sub@subfig3-5:fineTuning}{{b}{127}{Task performance with multi-task training}{figure.caption.58}{}}
\newlabel{subfig3-5:gmmb_individual}{{4.7c}{127}{GMMB individual task training}{figure.caption.58}{}}
\newlabel{sub@subfig3-5:gmmb_individual}{{c}{127}{GMMB individual task training}{figure.caption.58}{}}
\newlabel{subfig3-5:gmwb_individual}{{4.7d}{127}{GMWB individual task training}{figure.caption.58}{}}
\newlabel{sub@subfig3-5:gmwb_individual}{{d}{127}{GMWB individual task training}{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Multi-task Learning on \gls {rsgbm} GMMB and GMWB with dynamic lapse}}{127}{figure.caption.58}\protected@file@percent }
\newlabel{fig3:figure5}{{4.7}{127}{Multi-task Learning on \gls {rsgbm} GMMB and GMWB with dynamic lapse}{figure.caption.58}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Conclusion}{128}{section.4.4}\protected@file@percent }
\newlabel{sec3:conclusion}{{4.4}{128}{Conclusion}{section.4.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{129}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{5}{129}{Conclusion}{chapter.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Future Work: Deep Hedging Variable Annuities with Transfer Learning}{131}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:futureWork}{{6}{131}{Future Work: Deep Hedging Variable Annuities with Transfer Learning}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Markov Decision Process for Hedging VAs}{133}{section.6.1}\protected@file@percent }
\newlabel{sec:MDP}{{6.1}{133}{Markov Decision Process for Hedging VAs}{section.6.1}{}}
\citation{naic2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Hedging Environment of Variable Annuities}{134}{subsection.6.1.1}\protected@file@percent }
\newlabel{subsec:VASimulation}{{6.1.1}{134}{Hedging Environment of Variable Annuities}{subsection.6.1.1}{}}
\newlabel{eq3:lapse}{{6.1}{134}{Hedging Environment of Variable Annuities}{equation.6.1.1}{}}
\newlabel{eq3:subaccount}{{6.2}{134}{Hedging Environment of Variable Annuities}{equation.6.1.2}{}}
\citation{garleanu2013dynamic}
\newlabel{eq3:guarantee}{{6.3}{135}{Hedging Environment of Variable Annuities}{equation.6.1.3}{}}
\newlabel{eq3:withdrawal}{{6.4}{135}{Hedging Environment of Variable Annuities}{equation.6.1.4}{}}
\newlabel{eq3:lossGMMB}{{6.6}{135}{Hedging Environment of Variable Annuities}{equation.6.1.6}{}}
\newlabel{eq3:lossGMWB}{{6.7}{136}{Hedging Environment of Variable Annuities}{equation.6.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}State Space}{136}{subsection.6.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Action Space}{136}{subsection.6.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}Policy}{136}{subsection.6.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.5}Transition Probabilities}{137}{subsection.6.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.6}Reward and Discount Factor}{137}{subsection.6.1.6}\protected@file@percent }
\newlabel{eq3:rewardGMWB}{{6.9}{137}{Reward and Discount Factor}{equation.6.1.9}{}}
\newlabel{eq3:rewardGMMB}{{6.10}{137}{Reward and Discount Factor}{equation.6.1.10}{}}
\citation{silver2016mastering}
\citation{silver2016mastering}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.7}Value Function and Advantage Function}{138}{subsection.6.1.7}\protected@file@percent }
\newlabel{eq3:V_pi}{{6.11}{138}{Value Function and Advantage Function}{equation.6.1.11}{}}
\newlabel{eq3:Q_pi}{{6.12}{138}{Value Function and Advantage Function}{equation.6.1.12}{}}
\newlabel{eq3:A_pi}{{6.13}{138}{Value Function and Advantage Function}{equation.6.1.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Exising RL Algorithms and Their Limitations}{138}{section.6.2}\protected@file@percent }
\newlabel{sec:RLAlgorithms}{{6.2}{138}{Exising RL Algorithms and Their Limitations}{section.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Model-Based RL: From AlphaZero to Deep Hedging}{138}{subsection.6.2.1}\protected@file@percent }
\citation{sutton2018reinforcement}
\citation{buehler2019deep}
\citation{xu2020variable}
\citation{carbonneau2021deep}
\citation{buehler2019deep}
\citation{imaki2021no}
\citation{gatheral2022volatility}
\citation{horvath2021deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Model-Free RL for Hedging}{140}{subsection.6.2.2}\protected@file@percent }
\citation{hasbrouck1991measuring}
\citation{mnih2015human}
\citation{lin1992self}
\citation{kolm2019dynamic}
\citation{lillicrap2015continuous}
\citation{xu2022delta}
\citation{sutton1999policy}
\citation{buehler2019deep}
\citation{schulman2015trust}
\citation{schulman2017proximal}
\citation{du2020deep}
\citation{chong2023pseudo}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces Vanilla Policy Gradient (REINFORCE)}}{142}{algorithm.8}\protected@file@percent }
\newlabel{alg:REINFORCE}{{8}{142}{Vanilla Policy Gradient (REINFORCE)}{algorithm.8}{}}
\citation{schulman2015high}
\citation{schulman2015trust}
\citation{engstrom2020implementation}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Hedging VAs with a PPO Agent}{143}{section.6.3}\protected@file@percent }
\newlabel{sec:PPO}{{6.3}{143}{Hedging VAs with a PPO Agent}{section.6.3}{}}
\citation{ni2021recurrent}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}{\ignorespaces PPO for Hedging Variable Annuities}}{144}{algorithm.9}\protected@file@percent }
\newlabel{alg3:ppoHedging}{{9}{144}{PPO for Hedging Variable Annuities}{algorithm.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Accounting for Non-Markovian Dynamics}{144}{subsection.6.3.1}\protected@file@percent }
\newlabel{eq3:stateRNN}{{6.17}{144}{Accounting for Non-Markovian Dynamics}{equation.6.3.17}{}}
\citation{xiao2021optimal}
\@writefile{loa}{\contentsline {algorithm}{\numberline {10}{\ignorespaces Recurrent \gls {ppo} with \gls {lstm} for Hedging Variable Annuities}}{145}{algorithm.10}\protected@file@percent }
\newlabel{alg3:ppoHedging-rnn}{{10}{145}{Recurrent \gls {ppo} with \gls {lstm} for Hedging Variable Annuities}{algorithm.10}{}}
\newlabel{subfig3:architecturePPO}{{6.1a}{146}{regular \gls {ppo}}{figure.caption.59}{}}
\newlabel{sub@subfig3:architecturePPO}{{a}{146}{regular \gls {ppo}}{figure.caption.59}{}}
\newlabel{subfig3:architectureLSTMPPO}{{6.1b}{146}{Recurrent \gls {ppo} with \gls {lstm}}{figure.caption.59}{}}
\newlabel{sub@subfig3:architectureLSTMPPO}{{b}{146}{Recurrent \gls {ppo} with \gls {lstm}}{figure.caption.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces \gls {ppo} Network Architectures}}{146}{figure.caption.59}\protected@file@percent }
\newlabel{fig3:architecturePPO}{{6.1}{146}{\gls {ppo} Network Architectures}{figure.caption.59}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Transfer Learning for Hedging VAs}{146}{section.6.4}\protected@file@percent }
\newlabel{sec:TransferLearning}{{6.4}{146}{Transfer Learning for Hedging VAs}{section.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Reward Shaping}{146}{subsection.6.4.1}\protected@file@percent }
\citation{ng1999policy}
\citation{wiewiora2003principled}
\citation{harutyunyan2015expressing}
\citation{teh2017distral}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Policy Transfer}{147}{subsection.6.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}Evaluation Metrics}{147}{subsection.6.4.3}\protected@file@percent }
\citation{brockman2016openai}
\citation{paszke2019pytorch}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Numerical Experiments}{148}{section.6.5}\protected@file@percent }
\newlabel{sec:Experiments}{{6.5}{148}{Numerical Experiments}{section.6.5}{}}
\citation{engstrom2020implementation}
\citation{schulman2017proximal}
\citation{schulman2015high}
\citation{buehler2019deep}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces \gls {ppo} Hyperparameters and \gls {gmmb} Contract Specifications}}{149}{table.caption.60}\protected@file@percent }
\newlabel{tab3:hyperparameters}{{6.1}{149}{\gls {ppo} Hyperparameters and \gls {gmmb} Contract Specifications}{table.caption.60}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Hedging performance of deep hedging with transfer learning}}{150}{figure.caption.61}\protected@file@percent }
\newlabel{fig3:dh-transfers}{{6.2}{150}{Hedging performance of deep hedging with transfer learning}{figure.caption.61}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Hedging performance of recurrent \gls {ppo} and deep hedging}}{150}{figure.caption.62}\protected@file@percent }
\newlabel{fig3:ppo_dh}{{6.3}{150}{Hedging performance of recurrent \gls {ppo} and deep hedging}{figure.caption.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Hedging performance of standard \gls {ppo} and recurrent \gls {ppo}}}{151}{figure.caption.63}\protected@file@percent }
\newlabel{fig3:ppo_result}{{6.4}{151}{Hedging performance of standard \gls {ppo} and recurrent \gls {ppo}}{figure.caption.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Hedging performance of recurrent \gls {ppo} and delta hedging with different transaction costs}}{151}{figure.caption.64}\protected@file@percent }
\newlabel{fig3:ppo_cost}{{6.5}{151}{Hedging performance of recurrent \gls {ppo} and delta hedging with different transaction costs}{figure.caption.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Hedging performance of recurrent \gls {ppo} with or without anchor}}{152}{figure.caption.65}\protected@file@percent }
\newlabel{fig3:ppo_anchor}{{6.6}{152}{Hedging performance of recurrent \gls {ppo} with or without anchor}{figure.caption.65}{}}
\citation{rusu2015policy}
\citation{bengio2012deep}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Hedging performance of recurrent \gls {ppo} with \gls {gmwb}}}{153}{figure.caption.66}\protected@file@percent }
\newlabel{fig3:ppo_GMWB}{{6.7}{153}{Hedging performance of recurrent \gls {ppo} with \gls {gmwb}}{figure.caption.66}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Future Directions}{153}{section.6.6}\protected@file@percent }
\newlabel{sec:FutureDirections}{{6.6}{153}{Future Directions}{section.6.6}{}}
\bibstyle{apalike}
\bibdata{refP1,refP2,refP3,refOther}
\bibcite{ahmed2016survey}{{1}{2016}{{Ahmed et~al.}}{{}}}
\bibcite{andreas2017modular}{{2}{2017}{{Andreas et~al.}}{{}}}
\bibcite{artzner1999coherent}{{3}{1999}{{Artzner et~al.}}{{}}}
\bibcite{barton1998simulation}{{4}{1998}{{Barton}}{{}}}
\bibcite{bauer2008universal}{{5}{2008}{{Bauer et~al.}}{{}}}
\bibcite{bauer2012calculation}{{6}{2012}{{Bauer et~al.}}{{}}}
\bibcite{bellman1966dynamic}{{7}{1966}{{Bellman}}{{}}}
\bibcite{bengio2012deep}{{8}{2012}{{Bengio}}{{}}}
\bibcite{bengio2013representation}{{9}{2013}{{Bengio et~al.}}{{}}}
\bibcite{bengio1994learning}{{10}{1994}{{Bengio et~al.}}{{}}}
\@writefile{toc}{\contentsline {chapter}{\textbf  {References}}{155}{section*.67}\protected@file@percent }
\bibcite{bentley1975multidimensional}{{11}{1975}{{Bentley}}{{}}}
\bibcite{bishop2006pattern}{{12}{2006}{{Bishop and Nasrabadi}}{{}}}
\bibcite{bollerslev1990modelling}{{13}{1990}{{Bollerslev}}{{}}}
\bibcite{box1970distribution}{{14}{1970}{{Box and Pierce}}{{}}}
\bibcite{boyle2003guaranteed}{{15}{2003}{{Boyle and Hardy}}{{}}}
\bibcite{boyle1997reserving}{{16}{1997}{{Boyle and Hardy}}{{}}}
\bibcite{boyle1977equilibrium}{{17}{1977}{{Boyle and Schwartz}}{{}}}
\bibcite{broadie2015risk}{{18}{2015}{{Broadie et~al.}}{{}}}
\bibcite{brockman2016openai}{{19}{2016}{{Brockman et~al.}}{{}}}
\bibcite{brown2020language}{{20}{2020}{{Brown}}{{}}}
\bibcite{buehler2019deep}{{21}{2019}{{Buehler et~al.}}{{}}}
\bibcite{carbonneau2021deep}{{22}{2021}{{Carbonneau}}{{}}}
\bibcite{carlini2017towards}{{23}{2017}{{Carlini and Wagner}}{{}}}
\bibcite{caruana1997multitask}{{24}{1997}{{Caruana}}{{}}}
\bibcite{caruana2000overfitting}{{25}{2000}{{Caruana et~al.}}{{}}}
\bibcite{cathcart2015calculating}{{26}{2015}{{Cathcart et~al.}}{{}}}
\bibcite{chen2020note}{{27}{2020}{{Chen et~al.}}{{}}}
\bibcite{cheng2019fast}{{28}{2019}{{Cheng et~al.}}{{}}}
\bibcite{chong2023pseudo}{{29}{2023}{{Chong et~al.}}{{}}}
\bibcite{chung2014empirical}{{30}{2014}{{Chung et~al.}}{{}}}
\bibcite{cont2001empirical}{{31}{2001}{{Cont}}{{}}}
\bibcite{coppersmith1987matrix}{{32}{1987}{{Coppersmith and Winograd}}{{}}}
\bibcite{dang2021efficient}{{33}{2021}{{Dang}}{{}}}
\bibcite{dang2020efficient}{{34}{2020}{{Dang et~al.}}{{}}}
\bibcite{dang2022dynamic}{{35}{2022}{{Dang et~al.}}{{}}}
\bibcite{dang2023two}{{36}{2023}{{Dang et~al.}}{{}}}
\bibcite{devlin2018bert}{{37}{2018}{{Devlin}}{{}}}
\bibcite{du2020deep}{{38}{2020}{{Du et~al.}}{{}}}
\bibcite{eiopa2014underlying}{{39}{2014}{{EIOPA}}{{}}}
\bibcite{elman1990finding}{{40}{1990}{{Elman}}{{}}}
\bibcite{embrechts2013modelling}{{41}{2013}{{Embrechts et~al.}}{{}}}
\bibcite{engstrom2020implementation}{{42}{2020}{{Engstrom et~al.}}{{}}}
\bibcite{feng2020optimal}{{43}{2020}{{Feng and Song}}{{}}}
\bibcite{feng2016nested}{{44}{2016}{{Feng et~al.}}{{}}}
\bibcite{feng2022variable}{{45}{2022}{{Feng et~al.}}{{}}}
\bibcite{feng2017analytical}{{46}{2017}{{Feng and Jing}}{{}}}
\bibcite{fonseca2003simulation}{{47}{2003}{{Fonseca et~al.}}{{}}}
\bibcite{frazier2018bayesian}{{48}{2018}{{Frazier}}{{}}}
\bibcite{galton1886regression}{{49}{1886}{{Galton}}{{}}}
\bibcite{gan2013application}{{50}{2013}{{Gan}}{{}}}
\bibcite{gan2015valuation}{{51}{2015}{{Gan and Lin}}{{}}}
\bibcite{garleanu2013dynamic}{{52}{2013}{{G{\^a}rleanu and Pedersen}}{{}}}
\bibcite{gatheral2022volatility}{{53}{2022}{{Gatheral et~al.}}{{}}}
\bibcite{gers2000learning}{{54}{2000}{{Gers et~al.}}{{}}}
\bibcite{giles2015multilevel}{{55}{2015}{{Giles}}{{}}}
\bibcite{giles2019multilevel}{{56}{2019}{{Giles and Haji-Ali}}{{}}}
\bibcite{giurca2021delta}{{57}{2021}{{Giurca and Borovkova}}{{}}}
\bibcite{glasserman2004monte}{{58}{2004}{{Glasserman}}{{}}}
\bibcite{golestaneh2024many}{{59}{2024}{{Golestaneh et~al.}}{{}}}
\bibcite{goodfellow2016}{{60}{2016a}{{Goodfellow et~al.}}{{}}}
\bibcite{bengio2016}{{61}{2016b}{{Goodfellow et~al.}}{{}}}
\bibcite{goodfellow2014explaining}{{62}{2014}{{Goodfellow et~al.}}{{}}}
\bibcite{gordy2010nested}{{63}{2010}{{Gordy and Juneja}}{{}}}
\bibcite{bauer2015least}{{64}{2015}{{Ha and Bauer}}{{}}}
\bibcite{hamilton1989new}{{65}{1989}{{Hamilton}}{{}}}
\bibcite{hardle1990applied}{{66}{1990}{{H{\"a}rdle}}{{}}}
\bibcite{hardy2001regime}{{67}{2001}{{Hardy}}{{}}}
\bibcite{hardy2003investment}{{68}{2003}{{Hardy}}{{}}}
\bibcite{hardy2006introduction}{{69}{2006}{{Hardy}}{{}}}
\bibcite{hardy2022quantitative}{{70}{2022}{{Hardy and Saunders}}{{}}}
\bibcite{harutyunyan2015expressing}{{71}{2015}{{Harutyunyan et~al.}}{{}}}
\bibcite{hasbrouck1991measuring}{{72}{1991}{{Hasbrouck}}{{}}}
\bibcite{hastie2009elements}{{73}{2009}{{Hastie et~al.}}{{}}}
\bibcite{he2016deep}{{74}{2016}{{He et~al.}}{{}}}
\bibcite{heston1993closed}{{75}{1993}{{Heston}}{{}}}
\bibcite{hochreiter1997long}{{76}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{hong2014monte}{{77}{2014}{{Hong et~al.}}{{}}}
\bibcite{hong2017kernel}{{78}{2017}{{Hong et~al.}}{{}}}
\bibcite{hornik1989multilayer}{{79}{1989}{{Hornik et~al.}}{{}}}
\bibcite{horvath2021deep}{{80}{2021}{{Horvath et~al.}}{{}}}
\bibcite{hull2016options}{{81}{2016}{{Hull and Basu}}{{}}}
\bibcite{imaki2021no}{{82}{2021}{{Imaki et~al.}}{{}}}
\bibcite{jabbar2015methods}{{83}{2015}{{Jabbar and Khan}}{{}}}
\bibcite{jennen1988unifying}{{84}{1988}{{Jennen-Steinmetz and Gasser}}{{}}}
\bibcite{jeong2019improving}{{85}{2019}{{Jeong and Kim}}{{}}}
\bibcite{jiang2020beyond}{{86}{2020}{{Jiang et~al.}}{{}}}
\bibcite{jin2020deep}{{87}{2020}{{Jin et~al.}}{{}}}
\bibcite{kim2008ratio}{{88}{2008}{{Kim and Kutzner}}{{}}}
\bibcite{kingma2014adam}{{89}{2014}{{Kingma and Ba}}{{}}}
\bibcite{kleijnen2018design}{{90}{2018}{{Kleijnen}}{{}}}
\bibcite{kolm2019dynamic}{{91}{2019}{{Kolm and Ritter}}{{}}}
\bibcite{krizhevsky2012imagenet}{{92}{2012}{{Krizhevsky et~al.}}{{}}}
\bibcite{krizhevsky2017imagenet}{{93}{2017}{{Krizhevsky et~al.}}{{}}}
\bibcite{lebichot2021transfer}{{94}{2021}{{Lebichot et~al.}}{{}}}
\bibcite{lecun2015deep}{{95}{2015}{{LeCun et~al.}}{{}}}
\bibcite{lecun1998gradient}{{96}{1998}{{LeCun et~al.}}{{}}}
\bibcite{lieu2022adaptive}{{97}{2022}{{Lieu et~al.}}{{}}}
\bibcite{lillicrap2015continuous}{{98}{2015}{{Lillicrap et~al.}}{{}}}
\bibcite{lin1992self}{{99}{1992}{{Lin}}{{}}}
\bibcite{lin2020efficient}{{100}{2020a}{{Lin and Yang}}{{}}}
\bibcite{lin2020fast}{{101}{2020b}{{Lin and Yang}}{{}}}
\bibcite{liu2010stochastic}{{102}{2010}{{Liu and Staum}}{{}}}
\bibcite{longstaff2001valuing}{{103}{2001}{{Longstaff and Schwartz}}{{}}}
\bibcite{luo2016understanding}{{104}{2016}{{Luo et~al.}}{{}}}
\bibcite{mack1981local}{{105}{1981}{{Mack}}{{}}}
\bibcite{malmsten2010stylized}{{106}{2010}{{Malmsten and Ter{\"a}svirta}}{{}}}
\bibcite{marshall2010valuation}{{107}{2010}{{Marshall et~al.}}{{}}}
\bibcite{marukame2016error}{{108}{2016}{{Marukame et~al.}}{{}}}
\bibcite{mcculloch1943logical}{{109}{1943}{{McCulloch and Pitts}}{{}}}
\bibcite{mnih2015human}{{110}{2015}{{Mnih et~al.}}{{}}}
\bibcite{moerland2023model}{{111}{2023}{{Moerland et~al.}}{{}}}
\bibcite{nadaraya1964estimating}{{112}{1964}{{Nadaraya}}{{}}}
\bibcite{naic2021}{{113}{2021}{{NAIC}}{{}}}
\bibcite{nair2010rectified}{{114}{2010}{{Nair and Hinton}}{{}}}
\bibcite{neelakantan2015adding}{{115}{2015}{{Neelakantan et~al.}}{{}}}
\bibcite{ng1999policy}{{116}{1999}{{Ng et~al.}}{{}}}
\bibcite{ni2021recurrent}{{117}{2021}{{Ni et~al.}}{{}}}
\bibcite{nystrom1930praktische}{{118}{1930}{{Nystr{\"o}m}}{{}}}
\bibcite{chatgpt}{{119}{2023}{{OpenAI}}{{}}}
\bibcite{osfi2017life}{{120}{2017}{{OSFI}}{{}}}
\bibcite{pan2009survey}{{121}{2009}{{Pan and Yang}}{{}}}
\bibcite{paszke2019pytorch}{{122}{2019}{{Paszke et~al.}}{{}}}
\bibcite{scikit-learn}{{123}{2011}{{Pedregosa et~al.}}{{}}}
\bibcite{peng2020empirical}{{124}{2020}{{Peng and Nagata}}{{}}}
\bibcite{piscopo2011valuation}{{125}{2011}{{Piscopo and Haberman}}{{}}}
\bibcite{poole2014analyzing}{{126}{2014}{{Poole et~al.}}{{}}}
\bibcite{prechelt2002early}{{127}{2002}{{Prechelt}}{{}}}
\bibcite{stable-baselines3}{{128}{2021}{{Raffin et~al.}}{{}}}
\bibcite{rockafellar2002conditional}{{129}{2002}{{Rockafellar and Uryasev}}{{}}}
\bibcite{rosen2012metamodeling}{{130}{2012}{{Rosen et~al.}}{{}}}
\bibcite{rosenblatt1958perceptron}{{131}{1958}{{Rosenblatt}}{{}}}
\bibcite{ruf2022hedging}{{132}{2022}{{Ruf and Wang}}{{}}}
\bibcite{rumelhart1985learning}{{133}{1985}{{Rumelhart et~al.}}{{}}}
\bibcite{rusu2015policy}{{134}{2015}{{Rusu et~al.}}{{}}}
\bibcite{salle2014efficient}{{135}{2014}{{Salle and Y{\i }ld{\i }zo{\u {g}}lu}}{{}}}
\bibcite{scholkopf2002learning}{{136}{2002}{{Sch{\"o}lkopf and Smola}}{{}}}
\bibcite{schulman2015trust}{{137}{2015a}{{Schulman et~al.}}{{}}}
\bibcite{schulman2015high}{{138}{2015b}{{Schulman et~al.}}{{}}}
\bibcite{schulman2017proximal}{{139}{2017}{{Schulman et~al.}}{{}}}
\bibcite{seber2012linear}{{140}{2012}{{Seber and Lee}}{{}}}
\bibcite{shahriari2015taking}{{141}{2015}{{Shahriari et~al.}}{{}}}
\bibcite{silver2016mastering}{{142}{2016}{{Silver et~al.}}{{}}}
\bibcite{simonyan2014very}{{143}{2014}{{Simonyan and Zisserman}}{{}}}
\bibcite{srivastava2014dropout}{{144}{2014}{{Srivastava et~al.}}{{}}}
\bibcite{stothers2010complexity}{{145}{2010}{{Stothers}}{{}}}
\bibcite{strassen1969gaussian}{{146}{1969}{{Strassen}}{{}}}
\bibcite{sutskever2014sequence}{{147}{2014}{{Sutskever et~al.}}{{}}}
\bibcite{sutton2018reinforcement}{{148}{2018}{{Sutton and Barto}}{{}}}
\bibcite{sutton1999policy}{{149}{1999}{{Sutton et~al.}}{{}}}
\bibcite{szeg1939orthogonal}{{150}{1939}{{Szeg}}{{}}}
\bibcite{szegedy2013intriguing}{{151}{2013}{{Szegedy et~al.}}{{}}}
\bibcite{tang2020deep}{{152}{2020}{{Tang et~al.}}{{}}}
\bibcite{teh2017distral}{{153}{2017}{{Teh et~al.}}{{}}}
\bibcite{geneva2013variable}{{154}{2013}{{The Geneva Association}}{{}}}
\bibcite{torres2017fault}{{155}{2017}{{Torres-Huitzil and Girau}}{{}}}
\bibcite{wang2022smooth}{{156}{2022}{{Wang et~al.}}{{}}}
\bibcite{watson1964smooth}{{157}{1964}{{Watson}}{{}}}
\bibcite{wiewiora2003principled}{{158}{2003}{{Wiewiora et~al.}}{{}}}
\bibcite{williams1989learning}{{159}{1989}{{Williams and Zipser}}{{}}}
\bibcite{wirch1999synthesis}{{160}{1999}{{Wirch and Hardy}}{{}}}
\bibcite{xiao2021optimal}{{161}{2021}{{Xiao et~al.}}{{}}}
\bibcite{xu2022delta}{{162}{2022}{{Xu and Dai}}{{}}}
\bibcite{xu2020variable}{{163}{2020}{{Xu}}{{}}}
\bibcite{yan2024comprehensive}{{164}{2024}{{Yan et~al.}}{{}}}
\bibcite{yang2018bit}{{165}{2018}{{Yang et~al.}}{{}}}
\bibcite{yosinski2014transferable}{{166}{2014}{{Yosinski et~al.}}{{}}}
\bibcite{zhang2018decoupling}{{167}{2018}{{Zhang et~al.}}{{}}}
\bibcite{zhang2022sample}{{168}{2022}{{Zhang et~al.}}{{}}}
\bibcite{zhang2021bootstrap}{{169}{2021}{{Zhang et~al.}}{{}}}
\citation{*}
\gdef \@abspage@last{184}
